{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MSiba/ColabRepo/blob/main/regressor_pwngc_index_based.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG45YqPiOwtZ"
      },
      "source": [
        "# Imports and Checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3osdH6pxxp3"
      },
      "outputs": [],
      "source": [
        "#!pip install numba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHQwxgQ9YjWB"
      },
      "outputs": [],
      "source": [
        "#!pip install bcolz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Er22_9ilYM9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93460366-b429-42ec-9f5b-24e7844c1aa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/extended_omw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw-1.4.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2021.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet31.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ],
      "source": [
        "# Imports \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch import nn, Tensor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "import math\n",
        "from typing import Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "#import bcolz\n",
        "\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import functools\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"all\")\n",
        "# nltk.download(\"stopwords\")\n",
        "# nltk.download(\"punkt\")\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from nltk import pos_tag, WordNetLemmatizer\n",
        "from pprint import pprint\n",
        "import string\n",
        "\n",
        "import bz2\n",
        "import pickle\n",
        "import _pickle as cPickle\n",
        "\n",
        "import numba\n",
        "from numba import jit\n",
        "from numba import njit\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use('seaborn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axg3vWr_YsZM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5c9d3ab-6c59-4ca2-b6fa-eb71d93a95e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.11.0+cu113\n"
          ]
        }
      ],
      "source": [
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZ2DJA3vYM9J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f5b2e3c-a217-4552-f818-0b12c59149cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# set seed to ensure the same initialization for every run\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkdoYU92V4dv"
      },
      "outputs": [],
      "source": [
        "# To prevent Colab from disconnection\n",
        "# while True:pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNHjzlbeyYk7"
      },
      "source": [
        "### Check which GPU Google Colab is offering me at the moment\n",
        "\n",
        "[Code Source](https://github.com/jeffheaton/present/blob/master/youtube/benchmark_colab.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Some checks\n",
        "\n",
        "!ipython kernelspec list\n",
        "import sys\n",
        "print(sys.version)\n",
        "print(sys.executable)"
      ],
      "metadata": {
        "id": "Qh251YqptZzh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f394f5f-37fd-4945-df69-8225d1fd14a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TerminalIPythonApp] WARNING | Subcommand `ipython kernelspec` is deprecated and will be removed in future versions.\n",
            "[TerminalIPythonApp] WARNING | You likely want to use `jupyter kernelspec` in the future\n",
            "Available kernels:\n",
            "  ir         /usr/local/share/jupyter/kernels/ir\n",
            "  python3    /usr/local/share/jupyter/kernels/python3\n",
            "3.7.13 (default, Apr 24 2022, 01:04:09) \n",
            "[GCC 7.5.0]\n",
            "/usr/bin/python3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVhmRXs-yX4H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c93a7da-a190-454e-907c-69f863ddcac3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun May  1 21:36:53 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    26W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9oR28SdzAb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca404e14-7952-45ab-cd36-e12787baec93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "# check how much RAM is available\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UEKy3KhzFQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "597aa26c-72f0-4db4-8471-3c6cfa4f57cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Architecture:        x86_64\n",
            "CPU op-mode(s):      32-bit, 64-bit\n",
            "Byte Order:          Little Endian\n",
            "CPU(s):              8\n",
            "On-line CPU(s) list: 0-7\n",
            "Thread(s) per core:  2\n",
            "Core(s) per socket:  4\n",
            "Socket(s):           1\n",
            "NUMA node(s):        1\n",
            "Vendor ID:           GenuineIntel\n",
            "CPU family:          6\n",
            "Model:               63\n",
            "Model name:          Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "Stepping:            0\n",
            "CPU MHz:             2299.998\n",
            "BogoMIPS:            4599.99\n",
            "Hypervisor vendor:   KVM\n",
            "Virtualization type: full\n",
            "L1d cache:           32K\n",
            "L1i cache:           32K\n",
            "L2 cache:            256K\n",
            "L3 cache:            46080K\n",
            "NUMA node0 CPU(s):   0-7\n",
            "Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt arat md_clear arch_capabilities\n"
          ]
        }
      ],
      "source": [
        "# list cpu architecture information \n",
        "!lscpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuYOEKv_V_oT"
      },
      "source": [
        "function ClickConnect(){\n",
        "    console.log(\"Clicked on connect button\"); \n",
        "    document.querySelector(\"colab-connect-button\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "AaNWxx1ZYM9J"
      },
      "source": [
        "# Utils Functions (Loading Data, Vocab, Numericalization, Input Preprocessing, Tag Decoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwR3TRG1YM9L"
      },
      "outputs": [],
      "source": [
        "def to_tensor(string_list):\n",
        "    \"\"\"\n",
        "    Transforms a list of strings into a list of floats. \n",
        "    After writing the list to the .csv file, the list is transformed to list of strings.\n",
        "    :param string_list: list of strings\n",
        "    :return: torch tensor of float32\n",
        "    \"\"\"\n",
        "    l_str = [] \n",
        "    for ele in string_list:\n",
        "        if ele[0] == \"[\":\n",
        "            l_str.append(ele[1:])\n",
        "        else:\n",
        "            if ele[-1] == \"]\":\n",
        "                l_str.append(ele[:-1])\n",
        "            else:\n",
        "                l_str.append(ele)\n",
        "\n",
        "    str_vec = \" \".join(l_str)\n",
        "    torch_labels = torch.tensor(list(map(float, str_vec.split(' '))), dtype=torch.float32)\n",
        "    return torch_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3g38XKKYM9L"
      },
      "outputs": [],
      "source": [
        "def parse_data(file):\n",
        "    \"\"\"\n",
        "    reads the stem word and the spatial tag of each token in the .csv file\n",
        "    :param file: the csv file for training\n",
        "    :return: List of training data of the form [[tokenized_sentence-1, spatial_tensors],\n",
        "                                                [tokenized_sentence-1, spatial_tensors], ...]\n",
        "    \"\"\"\n",
        "    with open(file, encoding='utf-8') as f:\n",
        "        examples = []\n",
        "        words = []\n",
        "        lemmas = []\n",
        "        synset_offset = []\n",
        "        idx = []\n",
        "        labels = []\n",
        "        for line in f:\n",
        "            # print(\"initial line \", line)\n",
        "            line = line.strip()\n",
        "            # print(\"STRIPPING line \", line)\n",
        "            if not line:\n",
        "                examples.append([lemmas, synset_offset, labels, idx])\n",
        "                words = []\n",
        "                lemmas = []\n",
        "                synset_offset = []\n",
        "                idx = []\n",
        "                labels = []\n",
        "            else:\n",
        "                columns = line.split()\n",
        "                # print(\"cols\", columns)\n",
        "                words.append(columns[0])\n",
        "                lemmas.append(columns[1])\n",
        "                synset_offset.append(columns[4])\n",
        "                idx.append(int(columns[5]))\n",
        "                lab = to_tensor(columns[-6:])\n",
        "                labels.append(lab)\n",
        "\n",
        "               \n",
        "\n",
        "        return examples"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_testing_data(file_path): # file_path = os.path.join(TESTING_PATH, \"/senseval2.pt\")\n",
        "  \"\"\"\n",
        "  parses the testing datasets, and prepares it for training\n",
        "  \"\"\"\n",
        "\n",
        "  data = torch.load(file_path)\n",
        " \n",
        "  extracted_data = []\n",
        "\n",
        "  for i, sent in enumerate(data):\n",
        "      tmp_lemma = []\n",
        "      tmp_tag = []\n",
        "      tmp_synset = []\n",
        "      tmp_idx = []\n",
        "      tmp_pos = []\n",
        "      for j, word in enumerate(sent): \n",
        "        tmp_lemma.append(word[1])\n",
        "        tmp_tag.append(word[5])\n",
        "        tmp_synset.append(word[3])\n",
        "        tmp_idx.append(word[4])\n",
        "        tmp_pos.append(word[2])\n",
        "      tmp_sent = [tmp_lemma, tmp_tag, tmp_synset, tmp_idx, tmp_pos]\n",
        "      extracted_data.append(tmp_sent)\n",
        "  return extracted_data"
      ],
      "metadata": {
        "id": "2XwxzY3hom77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNzbOazAYM9M"
      },
      "outputs": [],
      "source": [
        "def clean_untagged(data):\n",
        "    \"\"\"\n",
        "    cleans the parsed data from a csv to remove entries that do not contain tagged senses.\n",
        "    :param data: The parsed data\n",
        "    return: original, and clean data\n",
        "    \"\"\"\n",
        "    original_data = data\n",
        "    for entry in data:\n",
        "        \n",
        "        idx = [i for i, syn in enumerate(entry[1]) if syn == 'no-synset']\n",
        "\n",
        "        # remove those from the data\n",
        "        for s in reversed(idx):\n",
        "            del entry[0][s]\n",
        "            del entry[1][s]\n",
        "            del entry[2][s]\n",
        "\n",
        "    return original_data, data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gUGwUHKYM9N"
      },
      "outputs": [],
      "source": [
        "def data_id(data):\n",
        "    \"\"\"\n",
        "    assigns for each sentence in the data an id to ease the splittings for training/validation.\n",
        "    :param data: \n",
        "    :return data with ids\n",
        "    \"\"\"\n",
        "\n",
        "    # data_collector = {\"0\": [[], []], \"1\": [[],[]], ...}\n",
        "    data_collector = {}\n",
        "    for i, instance in enumerate(data):\n",
        "        data_collector[str(i)] = instance\n",
        "\n",
        "    return data_collector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ucyBqVlYM9N"
      },
      "outputs": [],
      "source": [
        "def load_vocab(data, embed_size=300):\n",
        "    \"\"\"\n",
        "    loads the vocabulary from the data (words of sentences) and assigns for each word a glove embedding.\n",
        "    :param data: words for training/validation/testing\n",
        "    :return embed_size: vector embedding size \n",
        "    \"\"\"\n",
        "\n",
        "    data = list(data)\n",
        "    \n",
        "    # insert all dataset vocabulary\n",
        "    dataset_vocab = []\n",
        "    \n",
        "    print(data[0])\n",
        "    if isinstance(data[0], str):\n",
        "        dataset_vocab = data\n",
        "    else:\n",
        "        for instance in data:            \n",
        "            dataset_vocab += instance[0]\n",
        "    \n",
        "    # remove duplicates\n",
        "    target_vocab = set(dataset_vocab)\n",
        "    \n",
        "    # generate weights matrix using glove\n",
        "    matrix_len = len(target_vocab)\n",
        "    \n",
        "    weights_matrix = np.zeros((matrix_len, embed_size))\n",
        "    \n",
        "    words_found = 0\n",
        "\n",
        "    for i, word in enumerate(target_vocab):\n",
        "        try:\n",
        "            weights_matrix[i] = glove[word]\n",
        "            words_found += 1\n",
        "        except KeyError:\n",
        "            weights_matrix[i] = np.random.normal(scale=0.6, size=(embed_size, ))\n",
        "    \n",
        "    return target_vocab, weights_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wenA6SRyYM9O"
      },
      "outputs": [],
      "source": [
        "def numericalize(tokens_list, vocab):\n",
        "    \"\"\"\n",
        "    gives a number to each word in the training/testing dataset, so that the sentences can be handled by torch using numbers.\n",
        "    :param tokens_list: a sentence as list of tokens\n",
        "    :param vocab: the vocabulary of all sentences in the training data.\n",
        "    \"\"\"\n",
        "    \n",
        "    str2num = {word: index for index, word in enumerate(vocab)}\n",
        "    num_list = []\n",
        "    for token in tokens_list:\n",
        "        num_list.append(str2num[token])\n",
        "        \n",
        "    return torch.tensor(num_list, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xVEpzceYM9P"
      },
      "outputs": [],
      "source": [
        "# Preprocess Input sentence\n",
        "\n",
        "# set of english stop words U set of punctuation\n",
        "EN_STOPWORDS_PUNCT = set(stopwords.words('english')).union(set(string.punctuation))\n",
        "WN_LEMMATIZER = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def tags4wn(tag):\n",
        "    \"\"\"Penn Treebank tags: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
        "    Converts PennTreeBank tags to WN tags, e.g. n, a, v\"\"\"\n",
        "    tag_conversion = {\"NN\": \"n\", # noun\n",
        "                      \"JJ\": \"a\", # adjective\n",
        "                      \"VB\": \"v\", # verb\n",
        "                      \"RB\": \"r\"} # adverb\n",
        "    try:\n",
        "        # return the WN tags\n",
        "        return tag_conversion[tag[:2]]\n",
        "    except:\n",
        "        # if no tag is found, treat the word as a noun\n",
        "        return \"n\"\n",
        "\n",
        "def preprocess(sentence):\n",
        "    \"\"\"Preprocesses a raw input sentence and return a list of each word with its POS tag.\"\"\"\n",
        "    # Tokenization\n",
        "    tokenized_sentence = nltk.word_tokenize(sentence)\n",
        "    # lowercase all words\n",
        "    lower = [word.lower() for word in tokenized_sentence]\n",
        "    # print(lower)\n",
        "    # delete stop words and punctuation\n",
        "    clean_sentence = [word for word in lower if word not in EN_STOPWORDS_PUNCT]\n",
        "    # print(clean_sentence)\n",
        "    # use wordNet Lemmatizer to do POS and then lemmatize\n",
        "    pos_tagging = pos_tag(clean_sentence)\n",
        "    # print(pos_tagging)\n",
        "    # Lemmatize\n",
        "    lemmatized_sentence = [(WN_LEMMATIZER.lemmatize(word, pos=tags4wn(tag)), tags4wn(tag)) for word, tag in pos_tagging]\n",
        "    # print(lemmatized_sentence)\n",
        "\n",
        "    return lemmatized_sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysrHv73MEtCd"
      },
      "outputs": [],
      "source": [
        "def decode_tag(tokens, sentag_idx, sentag_dist, file, true_synsets=None):\n",
        "    \"\"\"\n",
        "    decodes and prints the output of the vicinity matrix in a readable way.\n",
        "    :param tokens: the words in the sentence to be tagged\n",
        "    :param sentag_idx: the indices of the senses in the vicinity matrix\n",
        "    :param sentag_dist: the distances to the senses in the vicinity matrix\n",
        "    :param file: file to which print() is redirected\n",
        "    :param true_synsets: the gold annotation of the word\n",
        "    :return a print of the vicinity matrix \n",
        "    \"\"\"\n",
        "\n",
        "    # Vicinity Matrix decoding\n",
        "    print(\"Please refer to the following descriptions to encode the relation between the predicted sphere and its vicinity.\", file=file)\n",
        "    print(\"part_of: The predicted sphere is fully part of in the fixed sphere.\", file=file)\n",
        "    print(\"near_to: The distance between the predicted sphere and the true sphere is minimal.\", file=file)\n",
        "    print(\"-\" * 100, file=file)\n",
        "    print(' ', file=file)\n",
        "    print('#' * 100, file=file)\n",
        "    print(' ', file=file)\n",
        "    \n",
        "    if true_synsets==None:\n",
        "      print(f\"{'token':8s} | {'kind':4s} | {'top_k':4s} | {'dist':4s}\", file=file)\n",
        "      print(' ', file=file)\n",
        "\n",
        "      for t, i, d in zip(tokens, sentag_idx, sentag_dist):\n",
        "        for (ik, iv), (dk, dv) in zip(i.items(), d.items()):\n",
        "          \n",
        "            # decode fct\n",
        "            iv, dv = iv.cpu().detach().numpy(), dv.cpu().detach().numpy()\n",
        "            senses, distances = target_VOCAB[iv], dv\n",
        "            for sense, distance in zip(senses, distances):\n",
        "                print(f'{t:8s} | {ik:4s} | {sense[0]:4s}, {sense[1]:4s} | {distance}', file=file)\n",
        "            print(' ', file=file)\n",
        "            print(' ', file=file)\n",
        "    else: \n",
        "        print(f\"{'token':8s} | {'true_synset':8s} | {'kind':4s} | {'top_k':4s} | {'dist':4s}\", file=file)\n",
        "        print(' ', file=file)\n",
        "        for t, i, d in zip(tokens, sentag_idx, sentag_dist):\n",
        "          for (ik, iv), (dk, dv) in zip(i.items(), d.items()):\n",
        "              # decode fct\n",
        "              iv, dv = iv.cpu().detach().numpy(), dv.cpu().detach().numpy()\n",
        "              senses, distances = target_VOCAB[iv], dv\n",
        "              for sense, distance in zip(senses, distances):\n",
        "                  print(f'{t:8s} | {\" or \".join(true_synsets):8s} | {ik:4s} | {sense[0]:4s}, {sense[1]:4s} | {str(distance)}', file=file)\n",
        "              print(' ', file=file)\n",
        "              print(' ', file=file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_results(predictions, results_path, traindata_name, chkpt_name, testset_name, testset_id, document, target_vocab, spatial_tags, k=5, device=DEVICE):\n",
        "    \"\"\"Saves the results of the testing instances into txt files.\"\"\"\n",
        "\n",
        "    processed_instances = []\n",
        "    for idx, out in predictions.items():\n",
        "        # check if the text file already exists (to resume) \n",
        "    \n",
        "        for i, word_tag in enumerate(out):\n",
        "            \n",
        "            if not os.path.exists(os.path.join(results_path, traindata_name, chkpt_name)):\n",
        "                os.mkdir(os.path.join(results_path, traindata_name, chkpt_name))\n",
        "\n",
        "            if not os.path.exists(os.path.join(results_path, traindata_name, chkpt_name, testset_name)):\n",
        "                os.mkdir(os.path.join(results_path, traindata_name, chkpt_name, testset_name))\n",
        "              \n",
        "            outfile = os.path.join(results_path, traindata_name, chkpt_name, testset_name, f'{idx}_{i}.txt')\n",
        "\n",
        "            if os.path.exists(outfile):\n",
        "                print(\"Existing file in: \", outfile)\n",
        "                print(\"Resume Results ...\")\n",
        "                i += 1\n",
        "                \n",
        "            else: \n",
        "\n",
        "                f = open(outfile, 'w+')\n",
        "                print(\"Original Document:\", file=f)\n",
        "                print(document[int(idx)], file=f)\n",
        "                print(\" \", file=f)\n",
        "                lemmatized_sentence = testset_id[idx][0]\n",
        "                annotations = testset_id[idx][2]\n",
        "                pos = testset_id[idx][4]\n",
        "                for lemma, p, annotation in zip(lemmatized_sentence, pos, annotations):\n",
        "                  print(f'{lemma}, {p}, {annotation}', file=f)\n",
        "                print('-' * 100, file=f)\n",
        "                print(' ', file=f)\n",
        "                predicted_indices = []\n",
        "                distances = []\n",
        "                                    \n",
        "                vindices, vmat, vdist = vicinity_matrix(spatial_params=word_tag,\n",
        "                                                        pos=pos[i],\n",
        "                                                        target_vocab=target_vocab,\n",
        "                                                        spatial_tags=spatial_tags, \n",
        "                                                        pos_tags=pos_tags,\n",
        "                                                        k=k, device=device)\n",
        "                predicted_indices.append(vindices)\n",
        "                distances.append(vdist)\n",
        "              \n",
        "                decode_tag(tokens=[lemmatized_sentence[i]], sentag_idx=[vindices], sentag_dist=[vdist], file=f, true_synsets=annotations[i])\n",
        "\n",
        "        processed_instances.append(idx)\n",
        "    print(\"Following instances of the testing set are processed: {}\".format(processed_instances))\n",
        "    return processed_instances\n"
      ],
      "metadata": {
        "id": "ndRhA3wy-Ym2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "JXSQufUkYM9P"
      },
      "source": [
        "# Mount files to google drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quHX2Q3SZ2-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00974817-d2ce-4e66-8295-297dd73dcd6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjdxaS0_DHdt"
      },
      "outputs": [],
      "source": [
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVnUQgeYvXSX"
      },
      "source": [
        "\n",
        "# Paths for training on PWNGC (only)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxRS52zYuOiX"
      },
      "outputs": [],
      "source": [
        "resources_path = \"/content/drive/MyDrive/ColabNotebooks/wsd_resources/resources_factory\"\n",
        "pwngc_path = \"pwngc\"\n",
        "train_path_pwngc = os.path.join(resources_path, pwngc_path, \"idx_complete_pwngc4regressor.csv\")\n",
        "# resources_path + pwngc_path + train_path_pwngc\n",
        "idx_file = os.path.join(resources_path, pwngc_path, \"indexed_pwngc_id.pt\")\n",
        "glove_path = \"/content/drive/MyDrive/ColabNotebooks/wsd_resources/glove/\"\n",
        "CHECKPOINT_PATH = \"/content/drive/MyDrive/ColabNotebooks/wsd_resources/checkpoints\"\n",
        "TESTING_PATH = \"/content/drive/MyDrive/ColabNotebooks/wsd_resources/testing_datasets\"\n",
        "RESULTS_PATH = \"/content/drive/MyDrive/ColabNotebooks/wsd_resources/results/\"\n",
        "ANNOTATION_PATH = \"/content/drive/MyDrive/ColabNotebooks/wsd_resources/annotations/\"\n",
        "traindata_name=\"pwngc\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6PSPtIAvhvc"
      },
      "source": [
        "#  run this code only once if you're running this notebook for the first time\n",
        "## preprocessing and saving data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MkQDNifvebw"
      },
      "outputs": [],
      "source": [
        "# # parse training data through path\n",
        "data = parse_data(train_path_pwngc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYVUeuZ1-vhh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbd6bbea-7184-4752-f451-be4096c5b3f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The initial data: 186655\n",
            "There are 939 empty entries.\n",
            "After removing empty entries, the data is now 185716\n"
          ]
        }
      ],
      "source": [
        "print(\"The initial data: {}\".format(len(data)))\n",
        "\n",
        "# checking if there are empty entries\n",
        "empty_entries = 0\n",
        "for entry in data:\n",
        "  if entry == [[], [], [], []]:\n",
        "    empty_entries +=1 \n",
        "\n",
        "print(\"There are {} empty entries.\".format(empty_entries)) # 939\n",
        "\n",
        "# remove empty entries\n",
        "data = list(filter(([[], [], [], []]).__ne__, data))\n",
        "\n",
        "print(\"After removing empty entries, the data is now {}\".format(len(data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6RKWIGSvfns",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80668551-3b05-432a-8109-7169ff5b02b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are in total 532821 annotated synsets.\n",
            "\n",
            "Without redundancy, 65163 synsets remain.\n",
            "\n",
            "The frequency by which the synsets are mentioned: how many synset is mentioned how many times?\n"
          ]
        }
      ],
      "source": [
        "# count the number of unique synsets and their redundancy\n",
        "all_synsets = []\n",
        "for i, sentence in enumerate(data):\n",
        "  print(sentence)\n",
        "  all_synsets += sentence[1]\n",
        "print(\"There are in total {} annotated synsets.\".format(len(all_synsets)))\n",
        "print()\n",
        "print(\"Without redundancy, {} synsets remain.\".format(len(set(all_synsets))))\n",
        "print()\n",
        "print(\"The frequency by which the synsets are mentioned: how many synset is mentioned how many times?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwBqhDk2AteW"
      },
      "outputs": [],
      "source": [
        "# count the number of words in the sentences\n",
        "nb_words_sent = []\n",
        "for i in range(len(data)):\n",
        "  nb_words_sent.append(len(data[i][0]))\n",
        "nb_words_sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQApEvALBG-v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "a2d1404c-cd19-4d2d-bc29-e709f1eaaee4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "image/png": {
              "width": 510,
              "height": 359
            }
          }
        }
      ],
      "source": [
        "labels, counts = np.unique(nb_words_sent, return_counts=True)\n",
        "ticks = range(len(counts))\n",
        "plt.bar(ticks,counts, align='center', color='b', alpha=0.81)\n",
        "for index, data in enumerate(counts):\n",
        "    plt.text(x=index-0.45 , y =data+500 , s=f\"{data}\" , fontdict=dict(fontsize=9))\n",
        "#plt.tight_layout()\n",
        "plt.title('Number of sense-annotated words in PWNGC sentences.', fontweight='bold')\n",
        "plt.xlabel('Number of words')#, fontweight='bold')\n",
        "plt.ylabel('Number of sentences')#, fontweight='bold')\n",
        "plt.style.use('seaborn-bright')\n",
        "# plt.rcParams[\"figure.facecolor\"] = \"w\"\n",
        "\n",
        "plt.xticks(ticks, labels)\n",
        "\n",
        "plt.savefig(resources_path + '/' + pwngc_path + '/pwngc_word_per_sentence.svg', format='svg')\n",
        "\n",
        "# files.download(resources_path + pwngc_path + '/pwngc_word_per_sentence.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CYgY_S9FCKy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66209edf-21c4-4d23-cfbe-7ad94ab17504"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "532821"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "np.sum(nb_words_sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### transform to dataset id and save model"
      ],
      "metadata": {
        "id": "kGTLDq9fypf7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xA099-2C_vIt"
      },
      "outputs": [],
      "source": [
        "datasetID = data_id(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__xqqkHpADxn"
      },
      "outputs": [],
      "source": [
        "# # store the dataset arranged by ID in a .pt file\n",
        "# # Saving and loading data to/from .pt\n",
        "# # save\n",
        "torch.save(datasetID, idx_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4swbnCh2Cc7t"
      },
      "source": [
        "# --- restart from here \n",
        "# DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4X2mjZ8CfpH"
      },
      "outputs": [],
      "source": [
        "datasetID = torch.load(idx_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oa1Zf3BLCpEi"
      },
      "outputs": [],
      "source": [
        "# partition data in training/validation\n",
        "splittings = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6eXNB_DDj7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "528cc610-9c85-40a7-fd3a-ca81d44b253e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(176430, 9285)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# split training and validation data by ID\n",
        "\n",
        "S = len(datasetID.keys())\n",
        "N_train = int(S * 95 / 100)\n",
        "N_valid = int(S * 5 / 100)\n",
        "# N_test = 5\n",
        "N_train, N_valid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKaH9wE4GnAg"
      },
      "outputs": [],
      "source": [
        "# choose N training instances, randomly!\n",
        "splittings[\"train\"] = random.sample(list(datasetID), N_train)\n",
        "splittings[\"validate\"] = random.sample(list(set(datasetID) - set(splittings[\"train\"])), N_valid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9F0cjOwHwu5"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Prepares the dataset for the DataLoader.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, list_ids, path2data):\n",
        "        super().__init__()\n",
        "\n",
        "        self.list_ids = list_ids\n",
        "        self.path2data = path2data\n",
        "        self.dataset = torch.load(self.path2data)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"Total Number of samples.\"\n",
        "\n",
        "        return len(self.list_ids)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"Extracts one Example of data.\"\n",
        "\n",
        "        id = self.list_ids[index]\n",
        "        \n",
        "\n",
        "        # data\n",
        "        X = self.dataset[id][0]\n",
        "        tag_y = self.dataset[id][1]\n",
        "        y = self.dataset[id][2]\n",
        "        idx_y = self.dataset[id][3]\n",
        "\n",
        "        return X, y, tag_y, idx_y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TestingDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Preparing the testing dataset to pytorch's DataLoader.\n",
        "    \"\"\"\n",
        "    def __init__(self, list_ids, path2data):\n",
        "        super().__init__()\n",
        "\n",
        "        self.list_ids = list_ids\n",
        "        self.path2data = path2data\n",
        "        self.dataset = torch.load(self.path2data)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"Total Number of samples.\"\n",
        "\n",
        "        return len(self.list_ids)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"Extracts one Example of data.\"\n",
        "\n",
        "        id = self.list_ids[index]\n",
        "        \n",
        "        instance = self.dataset[id]\n",
        "\n",
        "        # data\n",
        "        X = instance[0]\n",
        "        tag_y = instance[2]\n",
        "        y = instance[1]\n",
        "        idx_y = instance[3]\n",
        "\n",
        "        return id, X, y, tag_y, idx_y"
      ],
      "metadata": {
        "id": "OyOxESE-NWdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "EkIQUo3sYM9U"
      },
      "source": [
        "# Downloading Glove and SPATIAL Tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWprbK9lYM9U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f3ecddb-d2c1-4dee-de55-29a25c0312c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2195846\n"
          ]
        }
      ],
      "source": [
        "# decompress the pkl file\n",
        "glove_comp = glove_path + '840B.300_glove.pkl' + '.pbz2'\n",
        "bz2_data = bz2.BZ2File(glove_comp, 'rb')\n",
        "glove = cPickle.load(bz2_data)\n",
        "\n",
        "#glove = pickle.load(open(f'{glove_path}/840B.300_glove.pkl', 'rb'))\n",
        "print(len(glove))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xcZ3cskYM9V"
      },
      "outputs": [],
      "source": [
        "target_VOCAB = np.load(f'{glove_path}WORDNET_VOCAB_exp_01.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qZ2mE4cYM9V"
      },
      "outputs": [],
      "source": [
        "SPATIAL_TAGS = np.load(f'{glove_path}WORDNET_SPATIAL_TAGS_exp_01.npy')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags = np.load(f\"{glove_path}WORDNET_pos.npy\")"
      ],
      "metadata": {
        "id": "LFRcniOcynjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "7e53kHoFYM9V"
      },
      "source": [
        "# The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZ71W2qsYM9V"
      },
      "outputs": [],
      "source": [
        "def create_emb_layer(weights_matrix, non_trainable=False):\n",
        "    \"\"\"\n",
        "    creates the embedding layer by matching the words in the input sentences to their glove embeddings in the weights matrix.\n",
        "    :param weights_matrix: the weight matrix of the vocabulary in the input data and the vector embeddings\n",
        "    :param non_trainable\n",
        "    : return embedding layer of the neural network, the number of embeddings, and the embedding dimension\n",
        "    \"\"\"\n",
        "    num_embeddings, embedding_dim = weights_matrix.shape\n",
        "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
        "    weights_matrix = torch.from_numpy(weights_matrix)\n",
        "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
        "    if non_trainable:\n",
        "        emb_layer.weight.requires_grad = False\n",
        "\n",
        "    return emb_layer, num_embeddings, embedding_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3iYdvPUYM9W"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    calculates the positional encoding added to the embedding layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model: np.ndarray, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r37R6ZW8YM9W"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderRegressor(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the Transformer's encoder for regression.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, weights_matrix:np.ndarray, \n",
        "                 ntoken: int, out_features: int, d_model: int, nhead: int, d_hid: int,\n",
        "                 nlayers: int, dropout: float = 0.5):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.model_type = 'Transformer'\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        \n",
        "        self.weights_matrix = weights_matrix\n",
        "        \n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(self.weights_matrix, True)\n",
        "        \n",
        "        # Multi-head attention mechanism is included in TransformerEncoderLayer\n",
        "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout) # activation\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers, norm=None)\n",
        "        \n",
        "\n",
        "        self.emb = nn.Embedding(ntoken, d_model) \n",
        "        self.out_features = out_features\n",
        "        \n",
        "        # Linear layer: returns the last hidden state of the encoder \n",
        "        self.fc = nn.Linear(d_model, embedding_dim)\n",
        "        \n",
        "\n",
        "        # Now, I need to have a Linear space that takes the whole/subset dataframe as input, extracts its spatial_context_vec,\n",
        "        # based on Glove-word-vector + spatial_point,\n",
        "        # get its spatial tags\n",
        "        # calculate distance loss between them\n",
        "        # do backprop! \n",
        "        # Nx300 into Nx227733: matmul product of two matrices Nx300 and 300x227733 --> Nx227733\n",
        "        # use the indices to get the synset names as well as the mapping to coordinates\n",
        "        # into Nx5: mapping to the coordinates\n",
        "        \n",
        "        self.output = nn.Linear(embedding_dim, 5)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.init_weights()\n",
        "              \n",
        "        # -------------------------------------\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        \"\"\"\n",
        "        initialize weights using uniform distribution\n",
        "        \"\"\"\n",
        "        initrange = 0.1\n",
        "        self.emb.weight.data.uniform_(-initrange, initrange)\n",
        "    \n",
        "\n",
        "    def forward(self, src):\n",
        "        \"\"\"\n",
        "        processes the numericalized words forward in the network.\n",
        "        : param src: Tensor of shape [sentence_len, batch_size]\n",
        "        : return regression output of shape [sentence_len, batch_size, spatial_params]\n",
        "        \"\"\"\n",
        "        \n",
        "\n",
        "        src = torch.mul(self.emb(src), math.sqrt(self.d_model))  \n",
        "\n",
        "        # normalize the src such that the values lie in the interval [-1,1]  \n",
        "        src = src / torch.linalg.norm(src)   \n",
        "        # normalize to get into the interval [0,1]\n",
        "        src = torch.add(src, 1) / 2\n",
        "\n",
        "        src = self.pos_encoder(src)\n",
        "        \n",
        "        \n",
        "        encoder_output = self.transformer_encoder(src)\n",
        "        \n",
        "        \n",
        "        linear_layer = self.fc(encoder_output)\n",
        "\n",
        "        context_vec = torch.sum(linear_layer, dim=1)\n",
        "        \n",
        "        # regression output\n",
        "        coordinates = self.output(context_vec)\n",
        "\n",
        "        # sig_cont = self.sigmoid(coordinates)\n",
        "\n",
        "        # apply logarithmic scaling to scale numbers between [0, 1] to spatial parameters\n",
        "        # out = torch.pow(10, 6.0 * sig_cont + 1.0) / 6.0\n",
        "        # out = torch.pow(10, 6.0 * coordinates + 1.0) / 6.0\n",
        "\n",
        "        abs_coordinates = torch.abs(coordinates)\n",
        "\n",
        "        return abs_coordinates \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "sp-hIJf-YM9Z"
      },
      "source": [
        "# Geometric Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUoDMuy2YM9b"
      },
      "outputs": [],
      "source": [
        "def coo2point(coo):\n",
        "    \"\"\"\n",
        "    transforms the spatial coordinates into points.\n",
        "    :param coo: list of lengths, angles, and radius.\n",
        "    :return sense point, center point\n",
        "    \"\"\"\n",
        "    # print(coo)\n",
        "    l0 = coo[0]\n",
        "    alpha = coo[1]\n",
        "    alpha_rad = alpha * math.pi / 180\n",
        "    l_i = coo[2]\n",
        "    beta_i = coo[3]\n",
        "    beta_i_rad = beta_i * math.pi / 180\n",
        "    r = coo[4]\n",
        "    \n",
        "    # np.cos() and np.sin() take angles in radian as params\n",
        "    center_pt = torch.tensor([l0 * math.cos(alpha_rad), l0 * math.sin(alpha_rad)], dtype=torch.float64, requires_grad=True)\n",
        "    sense_pt = center_pt + torch.tensor([l_i * math.cos(alpha_rad + beta_i_rad),\n",
        "                                     l_i * math.sin(alpha_rad + beta_i_rad)], dtype=torch.float64, requires_grad=True)\n",
        "    return sense_pt, center_pt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def distance_loss(pred_pt, original_pt, include_r=False, pt_sphere=False, device=DEVICE):\n",
        "    \"\"\"\n",
        "    Calculates the distance between two sense points, including/without radii.\n",
        "    :param pred_pt: predicted point by the regressor \n",
        "    :param original_pt: the true point\n",
        "    :param include_r: if set to true, include radius in the distance. \n",
        "                      It gives more freedom/tolerance degrees to the loss function. \n",
        "                      Loss is satisfied once the predicted point is part of original point.\n",
        "    :return: distance loss\n",
        "    \"\"\"\n",
        "    original_pt = original_pt.to(device)\n",
        "    pred_pt = pred_pt.to(device)\n",
        "    \n",
        "    \n",
        "    r1 = pred_pt[-1]\n",
        "    r2 = original_pt[-1]\n",
        "\n",
        "    pred_sense, pred_center = coo2point(pred_pt)\n",
        "    orig_sense, orig_center = coo2point(original_pt)\n",
        "\n",
        "    loss = torch.linalg.norm(torch.sub(pred_sense, orig_sense)) - r2\n",
        "    \n",
        "    # very strong assumption for the words that are not sense-tagged\n",
        "    # If I want more tolerance, I could neglect those tokens from the beginning\n",
        "    if torch.all(torch.eq(original_pt, torch.zeros(original_pt.size(0)).to(device)), dim=0):\n",
        "        return loss\n",
        "    \n",
        "    if pt_sphere:\n",
        "        dist = torch.linalg.norm(torch.sub(pred_sense, orig_sense)) + r2\n",
        "        return dist\n",
        "\n",
        "    \n",
        "    if include_r:\n",
        "        \n",
        "        tolerant_loss = r1 + loss - r2\n",
        "    \n",
        "        if tolerant_loss < 0:\n",
        "            tolerant_loss = 0.0\n",
        "        \n",
        "           \n",
        "        return tolerant_loss\n",
        "    \n",
        "    else:\n",
        "        return loss \n",
        "   \n",
        "\n",
        "\n",
        "def geometric_loss(pred_list, label_list, include_r=False, device=DEVICE):\n",
        "    \"\"\"\n",
        "    calculates the distance loss over all the words in the sentence.\n",
        "    :param pred_list: predicted spatial parameters\n",
        "    :param label_list: true spatial parameters\n",
        "    :param include_r: if set to true, include radius in the distance. \n",
        "                      It gives more freedom/tolerance degrees to the loss function. \n",
        "                      Loss is satisfied once the predicted point is part of original point.\n",
        "    :return geometric loss among all words in the sentence.\n",
        "    \"\"\"\n",
        "    \n",
        "    # assert that the two lists must be of equal size\n",
        "    pred_size = pred_list.size()[0]\n",
        "    lab_size = label_list.size()[0]\n",
        "    \n",
        "    assert pred_size == lab_size, \"predicted coordinates <{}> and true coordinates <{}> have different sizes\".format(pred_list, label_list)\n",
        "    \n",
        "    sentence_loss = 0.0\n",
        "    \n",
        "    # sum over all the tokens in the sentence\n",
        "    for i in range(pred_size):\n",
        "        sentence_loss += distance_loss(pred_list[i], label_list[i], include_r, device)\n",
        "        \n",
        "    return sentence_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "buL4R4UuYM9c"
      },
      "source": [
        "#  Sense Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HszRl34EYM9d"
      },
      "outputs": [],
      "source": [
        "def is_contained(pred, sphere_coo, compare_spheres=False):\n",
        "\n",
        "    pt, word = coo2point(pred)\n",
        "    sphere_sense, sphere_center = coo2point(sphere_coo)\n",
        "\n",
        "    pt_rad = pred[-1]\n",
        "    sphere_rad = sphere_coo[-1] # in angles\n",
        "    \n",
        "    \n",
        "    \n",
        "    if compare_spheres == False:\n",
        "        contained = (pt[0] - sphere_sense[0])**2 + (pt[1] - sphere_sense[1])**2 <= sphere_rad**2\n",
        "    else:\n",
        "        contained = pt_rad + torch.linalg.norm(pt - sphere_sense) - sphere_rad <= 0\n",
        "\n",
        "    if contained:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "    \n",
        "\n",
        "@jit(nopython=True)\n",
        "def vicinity_matrix(spatial_params, target_vocab: np.ndarray, spatial_tags: np.ndarray, k=5, device=DEVICE):#, include_sphere=True, include_r=True) -> [str]:\n",
        "    \"\"\"\n",
        "    Projects the predicted spatial parameters into the embedding space.\n",
        "    Returns the synsets in the vicinity of the projected point.\n",
        "    :param spatial_params:\n",
        "    :return: Vicinity matrix, synsets dict\n",
        "    \"\"\"\n",
        "\n",
        "    # send to GPU\n",
        "    spatial_params = spatial_params.to(device)\n",
        "\n",
        "\n",
        "    N = len(spatial_tags)\n",
        "    \n",
        "    #convert spatial_tags to tensor\n",
        "    spatial_tags = torch.from_numpy(spatial_tags).double().to(device)\n",
        "    \n",
        "    synsets = {} # sort from most specific to most general\n",
        "    \n",
        "    indices = {}\n",
        "\n",
        "    sense_pt, center_pt = coo2point(spatial_params)\n",
        "    \n",
        "    # ----------------------------------------------------------------------------------------------------------------\n",
        "    # Prepare distance and containment calculations\n",
        "    # ----------------------------------------------------------------------------------------------------------------\n",
        "    \n",
        "    # distance calculations\n",
        "    dist_spheres = torch.empty(N, device=device) \n",
        "    dist_pt_sphere = torch.empty(N, device=device) \n",
        "    dist_pts = torch.empty(N, device=device)\n",
        "    \n",
        "    for i, tag in enumerate(spatial_tags):\n",
        "        dist_spheres[i] = distance_loss(spatial_params, tag, include_r=True)\n",
        "        dist_pt_sphere[i] = distance_loss(spatial_params, tag, pt_sphere=True)\n",
        "        dist_pts[i] = distance_loss(spatial_params, tag, include_r=False)\n",
        "    \n",
        "    # containment calculations\n",
        "    full_contained = torch.empty(N, device=device) \n",
        "    part_contained = torch.empty(N, device=device)\n",
        "    disconnected = torch.empty(N, device=device) # handles points only\n",
        "    \n",
        "    for j, tag in enumerate(spatial_tags):\n",
        "        full_contained[j] = is_contained(spatial_params, tag, compare_spheres=True)\n",
        "        part_contained[j] = distance_loss(spatial_params, tag, include_r=True, device=device) > 0\n",
        "        disconnected[j] = ~ is_contained(spatial_params, tag, compare_spheres=True) # reverse the True <----> False\n",
        "    \n",
        "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "    # Initialize the Vicinity Matrix\n",
        "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "    \n",
        "    # row=3, col=3, topk=2, 2 indicates the column of indices and the distances\n",
        "    vicinity_matrix = torch.zeros((3,3, k, 2), device=device)\n",
        "    \n",
        "    ####################################################################################################################\n",
        "    # # Full contained + min dist between sense points\n",
        "    ####################################################################################################################\n",
        "    \n",
        "#     print(\"True elements\")\n",
        "    true_indices1 = (full_contained == True).nonzero(as_tuple=True)[0]\n",
        "#     print(true_indices1)\n",
        "    \n",
        "    if true_indices1.size(0) != 0:\n",
        "        dist1 = torch.index_select(dist_pts, 0, true_indices1)\n",
        "#         print(\"dist1\", dist1)\n",
        "#         print(\"k = \", k)\n",
        "        # sort in ascending order\n",
        "        # select top k \n",
        "        sort_dist1, sort_indices = torch.topk(dist1, k, largest=False)  \n",
        "#         print(\"SORTING\", sort_dist1, sort_indices)\n",
        "        synsets1 = np.take(target_vocab, sort_indices, 0)\n",
        "        synsets[\"A\"] = [synsets1, sort_dist1]\n",
        "        indices[\"A\"] = sort_indices\n",
        "        # index, distance (without synsets because this would result in conflicts for torch.tensor that do not support str)\n",
        "        vicinity_matrix[2][0] = torch.stack((sort_indices, sort_dist1), dim=1)\n",
        "    else:\n",
        "        pass\n",
        "    \n",
        "    \n",
        "    ####################################################################################################################\n",
        "    # # Partially contained + min dist between sense points\n",
        "    ####################################################################################################################\n",
        "    true_indices2 = (part_contained == True).nonzero(as_tuple=True)[0]\n",
        "#     print(\"True Indices 2\", true_indices2)\n",
        "    \n",
        "    if true_indices2.size(0) != 0:\n",
        "        dist1 = torch.index_select(dist_pts, 0, true_indices2)\n",
        "        # sort in ascending order\n",
        "        # select top k \n",
        "        sort_dist2, sort_indices2 = torch.topk(dist1, k, largest=False)     \n",
        "        synsets2 = np.take(target_vocab, sort_indices2, 0)\n",
        "#         print(\"synset 2\", synsets2)\n",
        "        synsets[\"B\"] = [synsets2, sort_dist2]\n",
        "        indices[\"B\"] = sort_indices2\n",
        "        # index, distance (without synsets because this would result in conflicts for torch.tensor that do not support str)\n",
        "        vicinity_matrix[2][1] = torch.stack((sort_indices2, sort_dist2), dim=1)\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "    ####################################################################################################################\n",
        "    # # Disconnected + min dist between spheres/point2sphere/sense points ---> acts as Nearest neighbor\n",
        "    ####################################################################################################################\n",
        "    # get indices, where disconnected is true\n",
        "    true_indices3 = (disconnected == True).nonzero(as_tuple=True)[0]\n",
        "#     print(\"True Indices 3\", true_indices3)\n",
        "\n",
        "    if true_indices3.size(0) != 0:\n",
        "        # get the distances at those indices\n",
        "        dist_spheres3 = torch.index_select(dist_spheres, 0, true_indices3)\n",
        "        dist_pt_sphere3 = torch.index_select(dist_pt_sphere, 0, true_indices3)\n",
        "        dist_pts3 = torch.index_select(dist_pts, 0, true_indices3)\n",
        "\n",
        "        # sort-select top k minimum distances\n",
        "        sort_dist_spheres3, sort_sph_indices3 = torch.topk(dist_spheres3, k, largest=False)\n",
        "        sort_dist_pt_sphere3, sort_pt_sph_indices3 = torch.topk(dist_pt_sphere3, k, largest=False)\n",
        "        sort_dist_pts3, sort_pts_indices3 = torch.topk(dist_pts3, k, largest=False)\n",
        "\n",
        "        # get their corresponding synsets\n",
        "        synsets30 = np.take(target_vocab, sort_sph_indices3, 0)\n",
        "        #print(\"synset30\", synsets30)\n",
        "        synsets[\"C\"] = [synsets30, sort_dist_spheres3]\n",
        "        indices[\"C\"] = sort_sph_indices3\n",
        "        \n",
        "        synsets31 = np.take(target_vocab, sort_pt_sph_indices3, 0)\n",
        "        synsets[\"D\"] = [synsets31, sort_dist_pt_sphere3]\n",
        "        indices[\"D\"] = sort_pt_sph_indices3\n",
        "        \n",
        "        synsets32 = np.take(target_vocab, sort_pts_indices3, 0)\n",
        "        synsets[\"E\"] = [synsets32, sort_dist_pts3]\n",
        "        indices[\"E\"] = sort_pts_indices3\n",
        "        \n",
        "        # insert them into the vicinity matrix    \n",
        "        vicinity_matrix[0][3] = torch.stack((sort_sph_indices3, sort_dist_spheres3), dim=1)\n",
        "        vicinity_matrix[1][3] = torch.stack((sort_pt_sph_indices3, sort_dist_pt_sphere3), dim=1)\n",
        "        vicinity_matrix[2][3] = torch.stack((sort_pts_indices3, sort_dist_pts3), dim=1)  \n",
        "    \n",
        "\n",
        "\n",
        "#     # get the spheres, where the point/point+radius is contained/overlaping/near\n",
        "\n",
        "#     # 1. check if the predicted point is contained in some sense\n",
        "#     contained = torch.empty(N)\n",
        "    \n",
        "#     for i, tag in enumerate(spatial_tags):\n",
        "#         contained[i] = is_contained(spatial_params, tag, compare_spheres=include_sphere)\n",
        "    \n",
        "#     # 2. For those synsets, which is the nearest synset point\n",
        "#     #use distance() to calculate distance between centers\n",
        "#     distances = torch.empty(N)\n",
        "#     for i, tag in enumerate(spatial_tags):\n",
        "#         distances[i] = distance_loss(spatial_params, tag, include_r=include_r)\n",
        "    \n",
        "#     # sort dist--> indices\n",
        "#     # check if for those distances the containment is true\n",
        "#     # if true: choose the one having min_dist as sense\n",
        "#     # top k senses must be stored in a dict \n",
        "    \n",
        "#     # check if for those distances the containment is false, then, only the radius is falsly predicted (not priority now)\n",
        "#     # if false and min_dist: choose it as potential sense\n",
        "    \n",
        "    \n",
        "\n",
        "#     # 3. If None of the synsets apply to that word sense\n",
        "#     # use sphere_dist to find the nearest sphere (most general synset), and assign it to that synset\n",
        "#     # (this maybe good for rare senses)\n",
        "#     # acts as a second chance\n",
        "#     rare_contained = torch.empty(N)\n",
        "#     rare_distances = torch.empty(N)\n",
        "#     for i, tag in enumerate(spatial_tags):\n",
        "#         rare_contained[i] = is_contained(spatial_params, tag, compare_spheres=False) #only consider sense point\n",
        "#         rare_distances[i] = distance_loss(spatial_params, tag, include_r=False)\n",
        "\n",
        "\n",
        "    return indices, vicinity_matrix, synsets\n",
        "\n",
        "def decode_key(key, mtx):\n",
        "    if key == \"A\":\n",
        "        return mtx[2, 0]\n",
        "    if key == \"B\":\n",
        "        return mtx[2, 1]\n",
        "    if key == \"C\":\n",
        "        return mtx[0, 2]\n",
        "    if key == \"D\":\n",
        "        return mtx[1, 2]\n",
        "    if key == \"E\":\n",
        "        return mtx[2, 2]\n",
        "    \n",
        "@jit(nopython=True)\n",
        "def label_in_vicinity(vicinity_matrix, vicinity_synsets, target_vocab, spatial_tags, true_label):\n",
        "    \n",
        "    checked_synsets = []\n",
        "    contained = []\n",
        "    checks = 0\n",
        "    predicted = []\n",
        "    distances = []\n",
        "    \n",
        "    in_vicinity = False\n",
        "    associated_syn = []\n",
        "    \n",
        "    # true label is either one of the possibilities [word, synset] or a randomly chosen one\n",
        "    \n",
        "    # induce subset of word-synset name \n",
        "    \n",
        "    #spatial_tags = torch.from_numpy(spatial_tags)\n",
        "    #idx_label = (spatial_tags == true_label).nonzero(as_tuple=True)[0]\n",
        "    # transform to numpy to \n",
        "    true_label = true_label.cpu().numpy()\n",
        "    true_label = np.array(true_label, dtype=np.float64)\n",
        "    # keep spatial tag an np.ndarray\n",
        "    rounded_l = np.round(true_label, decimals=2).cpu()\n",
        "    \n",
        "    if np.all(rounded_l == np.zeros(5)): #true_label): #torch.all(torch.eq(rounded_l, true_label)):\n",
        "        in_vicinity = False #True\n",
        "        associated_syn.append('no-synset')\n",
        "        return in_vicinity, associated_syn\n",
        "    \n",
        "    try:\n",
        "        # detecting the true label from the spatial_tags\n",
        "        idx = [[np.array_equal(rounded_l, tag) for tag in spatial_tags].index(True)]\n",
        "#         print(\"Found {} matching word-synset tags.\".format(len(idx)))\n",
        "        word_synset = target_vocab[idx] #list of list \n",
        "#         print(\"Matching word-synset\", word_synset)\n",
        "        # check if word_synset is within the vicinity matrix\n",
        "        if len(word_synset) != 0:\n",
        "            for e in word_synset:\n",
        "                for key, val in vicinity_synsets.items():\n",
        "#                     print(\"Searching in vicinity ... \")\n",
        "\n",
        "#                     print(\"Checking if true label is in vicinity ...\")\n",
        "                    checked_synsets.append(e)\n",
        "                    is_there = e[1] in val[:, 1]\n",
        "                    checks += 1\n",
        "                    contained.append(is_there)\n",
        "                    \n",
        "#                     print(\"1\")\n",
        "#                     print(checked_synsets)\n",
        "#                     print(checks)\n",
        "#                     print(contained)\n",
        "                    \n",
        "                    if is_there:\n",
        "#                         print(\"The main true label <{}> is in the vacinity of the predicted tag.\".format(e))\n",
        "                        idx_e = np.where(val[:, 1] == e[1])\n",
        "                        predicted.append(val[idx_e])\n",
        "#                         print(\"Predicted 1: \", predicted)\n",
        "                        distances.append(decode_key(key, vicinity_matrix)[idx_e][1])\n",
        "#                         print(\"Distances 1: \", distances)\n",
        "                    else:\n",
        "#                         print(\"The main true label is not in vicinity ... \")\n",
        "                        distances.append('no-distance')\n",
        "#                         print(\"Searching if alternative true label synsets are in vicinity ... \")\n",
        "                    # induce all the word-synset tuples that have same synset as true label.\n",
        "                    # This double check is necessary since I choose the spatial tags in the training data randomly sometimes.\n",
        "                    # get indices of all word-synsets sharing same synset (not same word)\n",
        "                    ix = np.where(target_vocab == [_, e[1]])[0] # add [0] to indicate only the row index, not the column\n",
        "#                     print(\"Indices \", ix)\n",
        "                    if len(ix) != 0:\n",
        "                        pos_syn = target_vocab[ix]\n",
        "                        \n",
        "#                         print(\"Possible synsets: \", pos_syn)\n",
        "#                         print(target_vocab[:10])\n",
        "                        for t in pos_syn:\n",
        "                            checks += 1\n",
        "                            checked_synsets.append(t)\n",
        "                            is_near = t[1] in val[:, -1]\n",
        "                            contained.append(is_near)\n",
        "#                             print(\"2\")\n",
        "#                             print(checked_synsets)\n",
        "#                             print(checks)\n",
        "#                             print(contained)\n",
        "                            if is_near == True:                                    \n",
        "#                                 print(\"... The word-synset <{}> is in the vicinity of the predicted tag.\".format(t))\n",
        "                                idx_t = np.where(val[:, -1] == t[1])\n",
        "                                predicted.append(val[idx_t])\n",
        "#                                 print(\"Predicted 2: \", predicted)\n",
        "                                distances.append(decode_key(key, vicinity_matrix)[idx_t][1])\n",
        "#                                 print(\"Distances 2: \", distances)\n",
        "                            else:\n",
        "                                distances.append('no-distance')\n",
        "                    else: \n",
        "                        print(\"... There are no other possibilites for word-synset <{}>\".format(e))\n",
        "                            \n",
        "        else:\n",
        "            print(\"Cannot find the suitable synset of this spatial tag!\")\n",
        "\n",
        "        \n",
        "    except ValueError as ve:\n",
        "        print(ve)\n",
        "#         print(\"Found no index for the true label. Something went wrong ...\")\n",
        "#         print(\"Comparing <true label = {}> with <rounded label = {}>\".format(true_label, rounded_l))\n",
        "    \n",
        "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "    # Statistics\n",
        "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "    \n",
        "#     print(\"~\" * 80)\n",
        "#     print(\"Statistics\")\n",
        "#     print(\"~\" * 80)\n",
        "    \n",
        "#     print(\"Predicted Spatial Tag = \", spatial_params)\n",
        "#     print(\"Checked Spatial Tag(s) ; contained? ; Predicted ; distances = ({}):\".format(len(checked_synsets)))\n",
        "    for s, c, p, d in zip(checked_synsets, contained, predicted, distances):\n",
        "        print(s, \";\", c, \";\", \"\\n\", p, \";\", d)\n",
        "        print(\"-\"*100)\n",
        "        \n",
        "#     print(\"True Spatial Tag(s) is in vicinity of predicted tag: \", contained)\n",
        "    contained_idx = np.where(np.array(contained) == True)\n",
        "    \n",
        "#     print(\"contained_idx\", contained_idx)\n",
        "#     print(\"checked_idx\", np.array(checked_synsets)[contained_idx])\n",
        "#     print(\"slice\", np.array(checked_synsets)[:, 1])\n",
        "#     print(\"check_slice\", np.array(checked_synsets)[:, 1][contained_idx])\n",
        "\n",
        "    if len(contained_idx[0]) > 0:\n",
        "#         print()\n",
        "#         print(contained_idx)\n",
        "        only_syn = set(np.array(checked_synsets)[contained_idx])#[:, 1])\n",
        "        associated_syn.append(only_syn)\n",
        "#         print(\"True Sense Tag(s) = ({}) --> \".format(len(only_syn)), only_syn)\n",
        "#         print(\"Prediction is correct!\")\n",
        "        in_vicinity = True\n",
        "#         print(\"Distance(predicted_sense, nearest_true_sense) = ({}): \".format(len(np.array(predicted)[contained_idx])))\n",
        "#         for p, d in zip(np.array(predicted), distances):\n",
        "#               print(p, d)\n",
        "              \n",
        "    else:\n",
        "#         print(\"Prediction is false ..\")\n",
        "#         print(\"All synsets in the vicinity of the predicted tag are not true senses ..\")\n",
        "#         print(\"Please check manually if the synsets in the vicinity are generalizations of the true labels.\")\n",
        "        in_vicinity = False\n",
        "        associated_syn.append(\"no-synset\")\n",
        "    \n",
        "    \n",
        "    return in_vicinity, associated_syn\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vR0knpCt5zf"
      },
      "source": [
        "# Sense Inference Unification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sK-iK6lt9XT"
      },
      "outputs": [],
      "source": [
        "def is_contained(pred, sphere_coo, compare_spheres=False):\n",
        "    \"\"\"\n",
        "    checks if the predicted sphere is contained in the sphere coordinate.\n",
        "    :param pred: predicted spatial params\n",
        "    :param sphere_coo: the sphere coordinates\n",
        "    :param compare_spheres: whether or not to compare the spheres when checking if it is contained.\n",
        "                            If set to true, check if the predicted sphere lies in the sphere_coo.\n",
        "                            If set to False, check if the center point of the predicted sphere lies within the sphere_coo.\n",
        "    :return contained or not contained\n",
        "    \"\"\"\n",
        "\n",
        "    pt, word = coo2point(pred)\n",
        "    sphere_sense, sphere_center = coo2point(sphere_coo)\n",
        "\n",
        "    pt_rad = pred[-1]\n",
        "    sphere_rad = sphere_coo[-1] # in angles   \n",
        "    \n",
        "    \n",
        "    if compare_spheres == False:\n",
        "        contained = (pt[0] - sphere_sense[0])**2 + (pt[1] - sphere_sense[1])**2 <= sphere_rad**2\n",
        "    else:\n",
        "        contained = pt_rad + torch.linalg.norm(pt - sphere_sense) - sphere_rad <= 0\n",
        "\n",
        "    if contained:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "    \n",
        "\n",
        "def vicinity_matrix(spatial_params, pos: str, target_vocab: np.ndarray, spatial_tags: np.ndarray, pos_tags: np.ndarray, k=5, device=DEVICE):\n",
        "    \"\"\"\n",
        "    Projects the predicted spatial parameters into the embedding space, and inspects the spheres (synsets) lying around the predicted spatial parametes in different ways, e.g. contained, overlapping, etc.\n",
        "    Returns the synsets in the vicinity of the projected point.\n",
        "    :param spatial_params:\n",
        "    :return: Vicinity matrix, synsets dict\n",
        "    \"\"\"\n",
        "    # send to GPU\n",
        "    spatial_params = spatial_params.to(device)\n",
        "\n",
        "    idx_tags = {}\n",
        "\n",
        "    if pos=='a' or pos=='s':\n",
        "        # extract the indices having same pos\n",
        "        valid_idx = np.where(pos_tags=='a')[0]\n",
        "        spatial_tags = spatial_tags[valid_idx]\n",
        "       \n",
        "    elif pos == 'n' or pos == 'v' or pos == 'r':\n",
        "        valid_idx = np.where(pos_tags==pos)[0]\n",
        "        spatial_tags = spatial_tags[valid_idx]\n",
        "       \n",
        "    else: \n",
        "        valid_idx = list(range(len(spatial_tags)))\n",
        "        pass\n",
        "        \n",
        "\n",
        "    N = len(spatial_tags)\n",
        "    \n",
        "    valid_idx = torch.from_numpy(valid_idx).int().to(device)\n",
        "\n",
        "    #convert spatial_tags to tensor\n",
        "    spatial_tags = torch.from_numpy(spatial_tags).double().to(device)\n",
        "    \n",
        "    synsets = {} # sort from most specific to most general\n",
        "    \n",
        "    indices = {}\n",
        "\n",
        "    sense_pt, center_pt = coo2point(spatial_params)\n",
        "    \n",
        "    # ----------------------------------------------------------------------------------------------------------------\n",
        "    # Prepare distance and containment calculations\n",
        "    # ----------------------------------------------------------------------------------------------------------------\n",
        "    \n",
        "    # distance calculations\n",
        "    dist_spheres = torch.empty(N, device=device) \n",
        "    dist_pt_sphere = torch.empty(N, device=device) \n",
        "    dist_pts = torch.empty(N, device=device)\n",
        "    \n",
        "    for i, tag in enumerate(spatial_tags):\n",
        "        dist_spheres[i] = distance_loss(spatial_params, tag, include_r=True)\n",
        "        dist_pt_sphere[i] = distance_loss(spatial_params, tag, pt_sphere=True)\n",
        "        dist_pts[i] = distance_loss(spatial_params, tag, include_r=False)\n",
        "    \n",
        "    # containment calculations\n",
        "    full_contained = torch.empty(N, device=device) \n",
        "    part_contained = torch.empty(N, device=device)\n",
        "    disconnected = torch.empty(N, device=device) # handles points only\n",
        "    \n",
        "    for j, tag in enumerate(spatial_tags):\n",
        "        full_contained[j] = is_contained(spatial_params, tag, compare_spheres=True)\n",
        "        part_contained[j] = distance_loss(spatial_params, tag, include_r=True, device=device) > 0\n",
        "\n",
        "        \n",
        "    full_contained = full_contained.bool()\n",
        "    part_contained = part_contained.bool()\n",
        "    disconnected = ~ full_contained.bool()\n",
        "    disconnected = disconnected.bool()\n",
        "\n",
        "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "    # Initialize the Vicinity Matrix\n",
        "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "    \n",
        "    # row=3, col=3, topk=2, 2 indicates the column of indices and the distances\n",
        "    vicinity_matrix = torch.zeros((3,3, k, 2), device=device)\n",
        "    \n",
        "    ####################################################################################################################\n",
        "    # # Full contained + min dist between sense points\n",
        "    ####################################################################################################################\n",
        "    \n",
        "    true_indices1 = (full_contained == True).nonzero(as_tuple=True)[0]\n",
        "    \n",
        "    if true_indices1.size(0) != 0:\n",
        "        dist1 = torch.index_select(dist_pts, 0, true_indices1)\n",
        "       \n",
        "        # sort in ascending order\n",
        "        # select top k \n",
        "        sort_dist1, sort_indices = torch.topk(dist1, k, largest=False)  \n",
        "\n",
        "        synsets[\"A\"] = sort_dist1\n",
        "        indices[\"A\"] = valid_idx[sort_indices] # sort_indices\n",
        "        # index, distance (without synsets because this would result in conflicts for torch.tensor that do not support str)\n",
        "        vicinity_matrix[2][0] = torch.stack((sort_indices, sort_dist1), dim=1)\n",
        "    else:\n",
        "        pass\n",
        "    \n",
        "    \n",
        "    ####################################################################################################################\n",
        "    # Partially contained + min dist between sense points\n",
        "    ####################################################################################################################\n",
        "    true_indices2 = (part_contained == True).nonzero(as_tuple=True)[0]\n",
        "    \n",
        "    if true_indices2.size(0) != 0:\n",
        "        dist1 = torch.index_select(dist_pts, 0, true_indices2)\n",
        "        # sort in ascending order\n",
        "        # select top k \n",
        "        sort_dist2, sort_indices2 = torch.topk(dist1, k, largest=False) \n",
        "        \n",
        "        synsets[\"B\"] = sort_dist2\n",
        "        indices[\"B\"] = valid_idx[sort_indices2] #sort_indices2\n",
        "        # index, distance (without synsets because this would result in conflicts for torch.tensor that do not support str)\n",
        "        vicinity_matrix[2][1] = torch.stack((sort_indices2, sort_dist2), dim=1)\n",
        "\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "    ####################################################################################################################\n",
        "    # # Disconnected + min dist between spheres/point2sphere/sense points ---> acts as Nearest neighbor\n",
        "    ####################################################################################################################\n",
        "    # get indices, where disconnected is true\n",
        "    true_indices3 = (disconnected == True).nonzero(as_tuple=True)[0]\n",
        "\n",
        "    if true_indices3.size(0) != 0:\n",
        "        # get the distances at those indices\n",
        "        dist_spheres3 = torch.index_select(dist_spheres, 0, true_indices3)\n",
        "        dist_pt_sphere3 = torch.index_select(dist_pt_sphere, 0, true_indices3)\n",
        "        dist_pts3 = torch.index_select(dist_pts, 0, true_indices3)\n",
        "\n",
        "        # sort-select top k minimum distances\n",
        "        sort_dist_spheres3, sort_sph_indices3 = torch.topk(dist_spheres3, k, largest=False)\n",
        "        sort_dist_pt_sphere3, sort_pt_sph_indices3 = torch.topk(dist_pt_sphere3, k, largest=False)\n",
        "        sort_dist_pts3, sort_pts_indices3 = torch.topk(dist_pts3, k, largest=False)\n",
        "\n",
        "        # get their corresponding synsets\n",
        "        synsets[\"C\"] = sort_dist_spheres3\n",
        "        indices[\"C\"] = valid_idx[sort_sph_indices3] # sort_sph_indices3\n",
        "        \n",
        "        synsets[\"D\"] = sort_dist_pt_sphere3\n",
        "        indices[\"D\"] = valid_idx[sort_pt_sph_indices3] # sort_pt_sph_indices3\n",
        "        \n",
        "        synsets[\"E\"] = sort_dist_pts3\n",
        "        indices[\"E\"] = valid_idx[sort_pts_indices3] # sort_pts_indices3\n",
        "        \n",
        "        # insert them into the vicinity matrix    \n",
        "        vicinity_matrix[0][2] = torch.stack((sort_sph_indices3, sort_dist_spheres3), dim=1)\n",
        "        vicinity_matrix[1][2] = torch.stack((sort_pt_sph_indices3, sort_dist_pt_sphere3), dim=1)\n",
        "        vicinity_matrix[2][2] = torch.stack((sort_pts_indices3, sort_dist_pts3), dim=1) \n",
        "    else:\n",
        "        pass\n",
        " \n",
        "\n",
        "    return indices, vicinity_matrix, synsets\n",
        "@jit(nopython=True)\n",
        "def decode_key(key, mtx):\n",
        "    if key == \"A\":\n",
        "        return mtx[2, 0]\n",
        "    if key == \"B\":\n",
        "        return mtx[2, 1]\n",
        "    if key == \"C\":\n",
        "        return mtx[0, 2]\n",
        "    if key == \"D\":\n",
        "        return mtx[1, 2]\n",
        "    if key == \"E\":\n",
        "        return mtx[2, 2]\n",
        "\n",
        "\n",
        "def label_in_vicinity(vicinity_matrix, vicinity_indices,\n",
        "                      target_vocab, spatial_tags, true_label_idx):\n",
        "    t0 = time.time()\n",
        "    \n",
        "    \n",
        "    checked_synsets = []\n",
        "    contained = []\n",
        "    checks = 0\n",
        "    predicted = []\n",
        "    distances = []\n",
        "    \n",
        "    in_vicinity = False\n",
        "    associated_syn = []\n",
        "\n",
        "    true_label = spatial_tags[true_label_idx]\n",
        "\n",
        "    \n",
        "    idx = true_label_idx\n",
        "    word_synset = target_vocab[idx] #list of list \n",
        "\n",
        "    # check if word_synset is within the vicinity matrix\n",
        "    if len(word_synset) != 0:\n",
        "        for e in word_synset:\n",
        "            for key, val in vicinity_indices.items(): #vicinity_indices {\"B\": tensor([84, 26, 79, 63, 37])}\n",
        "                \n",
        "\n",
        "                print(\"Checking if true label is in vicinity ...\")\n",
        "                checked_synsets.append(e)\n",
        "                idx2sense = target_vocab[val]\n",
        "                print(\"idx2sense 1: \", idx2sense)\n",
        "                is_there = e[1] in idx2sense[:, 1]\n",
        "                checks += 1\n",
        "                contained.append(is_there)\n",
        "                                \n",
        "                if is_there:\n",
        "                    print(\"The main true label <{}> is in the vacinity of the predicted tag.\".format(e))\n",
        "                    idx_e = np.where(idx2sense[:, 1] == e[1])\n",
        "                    predicted.append(idx2sense[idx_e])\n",
        "                    distances.append(decode_key(key, vicinity_matrix)[idx_e][1])\n",
        "                else:\n",
        "                    distances.append('no-distance')\n",
        "\n",
        "                # induce all the word-synset tuples that have same synset as true label.\n",
        "                # This double check is necessary since I choose the spatial tags in the training data randomly sometimes.\n",
        "                # get indices of all word-synsets sharing same synset (not same word)\n",
        "                ix = np.where(target_vocab == [_, e[1]])[0] # add [0] to indicate only the row index, not the column\n",
        "                if len(ix) != 0:\n",
        "                    pos_syn = target_vocab[ix]\n",
        "                    \n",
        "                    for t in pos_syn:\n",
        "                        checks += 1\n",
        "                        checked_synsets.append(t)\n",
        "                        is_near = t[1] in idx2sense[:, -1]\n",
        "                        contained.append(is_near)\n",
        "                        \n",
        "                        if is_near == True:                                    \n",
        "                            idx_t = np.where(idx2sense[:, -1] == t[1])\n",
        "                            predicted.append(idx2sense[idx_t])\n",
        "                            distances.append(decode_key(key, vicinity_matrix)[idx_t][1])\n",
        "                        else:\n",
        "                            distances.append('no-distance')\n",
        "                else: \n",
        "                    print(\"... There are no other possibilites for word-synset <{}>\".format(e))\n",
        "                        \n",
        "    else:\n",
        "        print(\"Cannot find the suitable synset of this spatial tag!\")\n",
        "\n",
        "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "    # Statistics\n",
        "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "    \n",
        "#     print(\"~\" * 80)\n",
        "#     print(\"Statistics\")\n",
        "#     print(\"~\" * 80)\n",
        "    \n",
        "#     print(\"Predicted Spatial Tag = \", spatial_params)\n",
        "#     print(\"Checked Spatial Tag(s) ; contained? ; Predicted ; distances = ({}):\".format(len(checked_synsets)))\n",
        "    for s, c, p, d in zip(checked_synsets, contained, predicted, distances):\n",
        "        print(s, \";\", c, \";\", \"\\n\", p, \";\", d)\n",
        "        print(\"-\"*100)\n",
        "        \n",
        "#     print(\"True Spatial Tag(s) is in vicinity of predicted tag: \", contained)\n",
        "    contained_idx = np.where(np.array(contained) == True)\n",
        "    \n",
        "#     print(\"contained_idx\", contained_idx)\n",
        "#     print(\"checked_idx\", np.array(checked_synsets)[contained_idx])\n",
        "#     print(\"slice\", np.array(checked_synsets)[:, 1])\n",
        "#     print(\"check_slice\", np.array(checked_synsets)[:, 1][contained_idx])\n",
        "\n",
        "    if len(contained_idx[0]) > 0:\n",
        "#         print()\n",
        "#         print(contained_idx)\n",
        "        only_syn = set(np.array(checked_synsets)[contained_idx])#[:, 1])\n",
        "        associated_syn.append(only_syn)\n",
        "#         print(\"True Sense Tag(s) = ({}) --> \".format(len(only_syn)), only_syn)\n",
        "#         print(\"Prediction is correct!\")\n",
        "        in_vicinity = True\n",
        "              \n",
        "    else:\n",
        "#         print(\"Prediction is false ..\")\n",
        "#         print(\"All synsets in the vicinity of the predicted tag are not true senses ..\")\n",
        "#         print(\"Please check manually if the synsets in the vicinity are generalizations of the true labels.\")\n",
        "        in_vicinity = False\n",
        "        associated_syn.append(\"no-synset\")\n",
        "\n",
        "    t1 = time.time()\n",
        "\n",
        "    print(\"It took {} to run the in_vicinity check\".format(t1-t0))\n",
        "    \n",
        "    \n",
        "    return in_vicinity #, associated_syn\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDJe7O3mYM9f"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    'Counts the parameters of the model to allow comparision between different models.'\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "aAF8zYXFYM9f"
      },
      "source": [
        "# Training / Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yT4POprfYM9f"
      },
      "outputs": [],
      "source": [
        "class RegTagger:\n",
        "    \"\"\"\n",
        "    The Regressor Tagger to train, validate, test and tag sentences.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, use_cuda, device):\n",
        "        super().__init__()\n",
        "        self.use_cuda = use_cuda\n",
        "        self.device = device\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "  \n",
        "    def train(self, batch_size: int, num_workers: int, max_epochs: int,\n",
        "              splittings: dict, path2data: str, data: list, embed_size: int,\n",
        "              target_vocab: list, spatial_tags: list,\n",
        "              chkpt_path: str, chkpt_name: str,\n",
        "              optimizer: str=\"sgd\",\n",
        "              k=5,\n",
        "              d_model=300, d_hid=2000, nlayers=2, nhead=2, dropout=0.2,\n",
        "              lr=5.0, gamma=0.95,\n",
        "              shuffle=True):\n",
        "        \"\"\"\n",
        "        trains the TransformerEncoderRegressor on the training data using pytorch's DataLoader.\n",
        "\n",
        "        :param batch_size: size of the batch to be fed to the network\n",
        "        :param num_workers: number of processors when training (only 0 while training on Windows)\n",
        "        :param max_epochs: maximum number of epochs\n",
        "        :param splittings: dictionary storing the indices of training/validation sentences\n",
        "        :param path2data: path where the training data .pt is stored, input to the DataLoader\n",
        "        :param data: list of the training data\n",
        "        :param embed_size: size of the word to vector embedding \n",
        "        :param target_vocab: list of all word-synset entries in WordNet\n",
        "        :param spatial_tags: list of all spatial tags for target_vocab\n",
        "        :param chkpt_path: string path to the storage of checkopoints\n",
        "        :param chkpt_name: name of the checkpoint to be created/resumed\n",
        "        :param optimizer: name of the optimizer used for training (in lower case)\n",
        "        :param k: number of top synsets in the vicinity matrix\n",
        "        :param d_model: dimension of the model\n",
        "        :param d_hid: dimension of hidden layers\n",
        "        :param nlayers: number of Encoder's layers\n",
        "        :param nhead: number of heads for the attention mechanism\n",
        "        :param dropout: dropout rate of weights\n",
        "        :param lr: learning rate\n",
        "        :param gamma: multiplicative factor of learning rate decay\n",
        "        :param shuffle: whether or not to shuffle the input sentences \n",
        "\n",
        "        :return train history, including training and validation losses.\n",
        "        \"\"\"\n",
        "      \n",
        "        if num_workers > 0:\n",
        "          pin_memory = True\n",
        "        else:\n",
        "          pin_memory = False\n",
        "        \n",
        "        # create batches        \n",
        "        # parameters\n",
        "        params = {'batch_size': batch_size, \n",
        "                  'shuffle': shuffle,\n",
        "                  'collate_fn': lambda x: x,\n",
        "                  'num_workers': num_workers, \n",
        "                  'pin_memory': pin_memory}\n",
        "\n",
        "        # Training and validation data generators\n",
        "        training_set = Dataset(splittings['train'], path2data)\n",
        "        training_generator = DataLoader(training_set, **params)\n",
        "\n",
        "        validation_set = Dataset(splittings['validate'], path2data)\n",
        "        validation_generator = DataLoader(validation_set, **params)\n",
        "\n",
        "        # -------------------------------------------------\n",
        "    \n",
        "        # history to store the losses\n",
        "        history = defaultdict(list)\n",
        "\n",
        "        # Load vocab\n",
        "        VOCAB, weights_matrix = load_vocab(data, embed_size=embed_size)\n",
        "\n",
        "    \n",
        "        #######################################################################################################################\n",
        "        #        Count sentences and number of words in training and validation datasets to normalize the loss\n",
        "        #######################################################################################################################\n",
        "        nb_words_training = 0\n",
        "        nb_train_sentences = 0\n",
        "        nb_words_validation = 0\n",
        "\n",
        "        for batch in training_generator:\n",
        "            for sentence, label, syn, idx in batch:\n",
        "                nb_train_sentences += 1\n",
        "                nb_words_training += len(sentence)\n",
        "\n",
        "        for batch in validation_generator:\n",
        "            for sentence, label, syn, idx in batch:\n",
        "                nb_words_validation += len(sentence)\n",
        "\n",
        "        print()\n",
        "        print(\"Count results:\")\n",
        "        print(\"nb_words_training = {}\".format(nb_words_training))\n",
        "        print(\"nb_train_sentences = {}\".format(nb_train_sentences))\n",
        "        print(\"nb_words_validation = {}\".format(nb_words_validation))\n",
        "        print()\n",
        "\n",
        "\n",
        "        n_batches = np.ceil(nb_train_sentences / batch_size)\n",
        "\n",
        "        mean_words = nb_words_training / n_batches\n",
        "        mean_words = torch.from_numpy(np.array(mean_words).astype('float')).double().to(self.device)\n",
        "\n",
        "\n",
        "        self.model = TransformerEncoderRegressor(weights_matrix = weights_matrix, \n",
        "                                            ntoken= len(VOCAB), \n",
        "                                            out_features=5,\n",
        "                                            d_model=d_model,\n",
        "                                            d_hid=d_hid,\n",
        "                                            nlayers=nlayers,\n",
        "                                            nhead=nhead,\n",
        "                                            dropout=dropout)\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        #                       Optimizer\n",
        "        # ---------------------------------------------------------------------\n",
        "        \n",
        "        criterion = nn.MSELoss()\n",
        "        if optimizer == \"SGD\":\n",
        "            optimizer = torch.optim.SGD(self.model.parameters(), lr=lr, momentum=0.9, nestorov=True)\n",
        "            \n",
        "        if optimizer == \"adam\":\n",
        "            optimizer = torch.optim.Adam(self.model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1.0, gamma=gamma)\n",
        "        # -------\n",
        "\n",
        "        starting_epoch = 0\n",
        "\n",
        "        # In case there is a checkpoint already, we can resume the training \n",
        "        if os.path.exists(os.path.join(chkpt_path, chkpt_name)):\n",
        "            print(\"Resuming Training in the file {} ...\".format(chkpt_name))\n",
        "            print()\n",
        "            checkpoint = torch.load(os.path.join(chkpt_path, chkpt_name), map_location=self.device)\n",
        "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            starting_epoch = checkpoint['epoch']\n",
        "            loss_sum = checkpoint['loss']\n",
        "            history = checkpoint['history']\n",
        "        else:\n",
        "          pass\n",
        "\n",
        "\n",
        "        # Loop over epochs\n",
        "        for epoch in range(starting_epoch, max_epochs):\n",
        "            # print(\"epoch = \", epoch)\n",
        "\n",
        "            t0 = time.time()\n",
        "\n",
        "            loss_sum = 0\n",
        "\n",
        "            self.model.train()\n",
        "\n",
        "            # for optimizer\n",
        "            scheduler.step()\n",
        "\n",
        "            # saves the checkpoint file\n",
        "            torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss_sum,\n",
        "            'history': history\n",
        "            }, os.path.join(chkpt_path, chkpt_name))\n",
        "\n",
        "\n",
        "            print(\"Training ...\")\n",
        "            # Training\n",
        "            for batch in training_generator:\n",
        "\n",
        "                for local_batch, local_labels, local_synsets, local_idx in batch:\n",
        "\n",
        "                    # Transform list(<string>) to Tensor(<Tensor>) after numerecalization \n",
        "                    input_words = local_batch\n",
        "                    local_batch = numericalize(local_batch, VOCAB)\n",
        "\n",
        "                    extract_labels = spatial_tags[local_idx]\n",
        "                    local_labels = [torch.tensor(lab) for lab in extract_labels]\n",
        "\n",
        "                    \n",
        "                    # Transform List(<Tensor>) to Tensor(<Tensor>)\n",
        "                    local_labels = torch.stack(local_labels)\n",
        "                    \n",
        "\n",
        "                    # Transfer to GPU\n",
        "                    local_batch, local_labels = local_batch.to(self.device), local_labels.to(self.device)\n",
        "\n",
        "                    # Model computations\n",
        "                    # out outputs the predicted spatial parameters\n",
        "                    out = self.model(local_batch)\n",
        "                    \n",
        "                    loss = geometric_loss(out, local_labels, device=self.device) / mean_words\n",
        "                    \n",
        "                    # backpropagate\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
        "                    # ---\n",
        "\n",
        "                    optimizer.step()\n",
        "                    loss_sum += loss.item()\n",
        "\n",
        "\n",
        "            train_loss = loss_sum / n_batches\n",
        "            history['train_loss'].append(train_loss)\n",
        "\n",
        "            # Evaluate on the validation set.\n",
        "            # evaluate every 1 step:\n",
        "            print(\"Validation ...\")\n",
        "            vloss_sum = 0\n",
        "            if epoch % 1 == 0:\n",
        "\n",
        "                correct_sense = 0\n",
        "                sense_accuracy = 0\n",
        "\n",
        "                # set model to eval mode to ignore updating the weights of the model\n",
        "                self.model.eval()\n",
        "\n",
        "                # do not calculate gradients while evaluating\n",
        "                with torch.set_grad_enabled(False):\n",
        "\n",
        "                    for batch in validation_generator:\n",
        "\n",
        "                        for local_batch, local_labels, local_synsets, local_idx in batch:\n",
        "\n",
        "                            # Transform list(<string>) to Tensor(<Tensor>)\n",
        "                            input_words = local_batch\n",
        "                            local_batch = numericalize(local_batch, VOCAB)\n",
        "\n",
        "                            extract_labels = spatial_tags[local_idx]\n",
        "                            local_labels = [torch.tensor(lab) for lab in extract_labels]\n",
        "\n",
        "\n",
        "                            # Transform List(<Tensor>) to Tensor(<Tensor>)\n",
        "                            # I have labels of same length --> this should be no problem for Tensor\n",
        "                            local_labels = torch.stack(local_labels)\n",
        "                            \n",
        "\n",
        "                            # Transfer to GPU\n",
        "                            local_batch, local_labels = local_batch.to(self.device), local_labels.to(self.device)\n",
        "                           \n",
        "                            # Model computations\n",
        "                            # out outputs the spatial params\n",
        "                            out = self.model(local_batch)\n",
        "\n",
        "                            # During validation and testing, I want to be less strict.\n",
        "                            # So, if a point resides within the label sphere, the sense is correctly identified.\n",
        "                            loss = geometric_loss(out, local_labels, include_r=False) / nb_words_validation\n",
        "\n",
        "                            vloss_sum += loss.item()                  \n",
        "\n",
        "                      \n",
        "#                             correct_sense_batch = 0\n",
        "# #                             print(\"Initializing the corrext sense batch = {}\".format(correct_sense_batch))\n",
        "\n",
        "#                             true_pred = []\n",
        "#                             # predicted_synsets = []\n",
        "\n",
        "#                             for i, word_tag in enumerate(out):\n",
        "#                                 #print(\"I am in the word tag loop : i = \", i)\n",
        "# #                                 print(\"+\"*150)\n",
        "# #                                 print(\"word_tag = \", word_tag.size())\n",
        "# #                                 print(word_tag)\n",
        "# #                                 print(\"+\"*150)\n",
        "\n",
        "\n",
        "#                                 vindices, vmat, vsyn = vicinity_matrix(spatial_params=word_tag,\n",
        "#                                                                target_vocab=target_vocab,\n",
        "#                                                                spatial_tags=spatial_tags, \n",
        "#                                                                k=k, device=self.device)\n",
        "#                                 # print(\"Vicinity Matrix-Synsets: {}\".format(vmat))\n",
        "#                                 # print(\"vmat\", vmat.get_device())\n",
        "#                                 # print(\"I passed the vicinity_matrix function\")\n",
        "\n",
        "#                                 #numba_type_indices = numba.typeof(np.array(k, dtype=np.int64))\n",
        "#                                 numba_indices = numba.typed.Dict.empty(\n",
        "#                                     key_type=numba.core.types.unicode_type,\n",
        "#                                     value_type=numba.int64[:],\n",
        "#                                     )\n",
        "#                                 # The typed-dict can be used from the interpreter.\n",
        "#                                 for key, value in vindices.items():\n",
        "#                                   # print(\"The Value\", type(value.cpu().detach().numpy()), value.cpu().detach().numpy())\n",
        "#                                   # print(numba.typeof(value.cpu().detach().numpy()))\n",
        "#                                     numba_indices[key] = np.asarray(value.cpu().detach().numpy(), dtype='i8')\n",
        "#                                 #print(\"numba indices\", numba_indices)\n",
        "#                                 #numba_indices['posx'] = np.asarray([1, 0.5, 2], dtype='f8')                                \n",
        "\n",
        "\n",
        "#                                 in_vic = label_in_vicinity(vicinity_matrix=vmat.cpu().detach().numpy(), #vicinity_synsets=vsyn,\n",
        "#                                                            vicinity_indices=numba_indices,\n",
        "#                                                            target_vocab=target_vocab, \n",
        "#                                                            spatial_tags=spatial_tags, \n",
        "#                                                            #true_label=local_labels[i].cpu().detach().numpy()\n",
        "#                                                            true_label_idx = local_idx\n",
        "#                                                            )\n",
        "                                \n",
        "#                                 # print(\"I passed the label_in_vicinity function\")\n",
        "#                                 # print(\"in_vic?\", in_vic)\n",
        "#                                 # print(\"pred_syn\", pred_syn)\n",
        "\n",
        "#                                 true_pred.append(in_vic)\n",
        "#                                 # predicted_synsets.append(pred_syn)\n",
        "                                \n",
        "#                                 print(\"In Vicinity? --> {}\".format(in_vic))\n",
        "# #                                 print(\"Predicted synsets --> {}\".format(pred_syn))\n",
        "\n",
        "#                                 if in_vic==True:\n",
        "#                                     correct_sense += 1\n",
        "#                                     correct_sense_batch += 1\n",
        "\n",
        "#                             # print(true_pred)\n",
        "#                             # print(predicted_synsets)\n",
        "                        \n",
        "#                             batch_acc = correct_sense_batch / len(local_batch)\n",
        "#                             history[\"sense_accuracy\"].append(batch_acc)\n",
        "# #                             print(\"correct sense batch ({}) / local_batch ({}) = {}\".format(correct_sense_batch, len(local_batch), batch_acc))\n",
        "\n",
        "                # sense_accuracy = correct_sense / nb_words_validation * 100\n",
        "\n",
        "                # print(f\"The sense accuracy on the validation set is {sense_accuracy:.3f}%\")   #.format(sense_accuracy * 100))\n",
        "                \n",
        "                validation_loss = vloss_sum / n_batches\n",
        "                history['validation_loss'].append(validation_loss)\n",
        "                    \n",
        "            t1 = time.time()\n",
        "            print(f'Epoch {epoch+1}: train loss = {train_loss:.4f}, time = {t1-t0:.4f} (s)')\n",
        "\n",
        "\n",
        "        return history\n",
        "    \n",
        "    \n",
        "       # assuming the sentence is already splitted into tokens, e.g. ['fall', 'in', 'catastrophes']\n",
        "    def test(self, testing_data, splittings, path, embed_size, batch_size, num_workers, target_vocab, spatial_tags, k=5, shuffle=True):\n",
        "        \"\"\"\n",
        "        tests the TransformerEncoderRegressor on the testing data using pytorch's DataLoader.\n",
        "        :param testing_data:\n",
        "        :param splittings: dictionary storing the indices of training/validation sentences\n",
        "        :param path: path where the testing data .pt is stored, input to the DataLoader\n",
        "        :param embed_size: size of the word to vector embedding \n",
        "        :param batch_size: size of the batch to be fed to the network\n",
        "        :param num_workers: number of processors when training (only 0 while training on Windows) \n",
        "        :param target_vocab: list of all word-synset entries in WordNet\n",
        "        :param spatial_tags: list of all spatial tags for target_vocab\n",
        "        :param k: number of top synsets in the vicinity matrix\n",
        "        :param shuffle: whether or not to shuffle the input sentences \n",
        "\n",
        "        :return testing history, including testing loss\n",
        "        \"\"\"\n",
        "        \n",
        "        if num_workers > 0:\n",
        "          pin_memory = True\n",
        "        else:\n",
        "          pin_memory = False\n",
        "\n",
        "        # parameters\n",
        "        params = {'batch_size': batch_size, #64,\n",
        "                  'shuffle': shuffle,\n",
        "                  'collate_fn': lambda x: x,\n",
        "                  'num_workers': num_workers, #6} #set 0 if training on Windows machine\n",
        "                  'pin_memory': pin_memory}\n",
        "        \n",
        "        # testing data generator\n",
        "        testing_set = TestingDataset(splittings, path)\n",
        "        testing_generator = DataLoader(testing_set, **params)\n",
        "\n",
        "        # history to store testing loss\n",
        "        history = defaultdict(list)\n",
        "        \n",
        "        # ------\n",
        "        # Count words in sentence to calculate accuracy\n",
        "        # ------\n",
        "        nb_words_testing = 0\n",
        "\n",
        "        for batch in testing_generator:\n",
        "            for id, sentence, label, syn, idx in batch:\n",
        "                nb_words_testing += len(sentence)\n",
        "        print()\n",
        "        print(\"Number of words to be tested: \", nb_words_testing)\n",
        "        print()\n",
        "                \n",
        "        # --------------------------\n",
        "        VOCAB, weights_matrix = load_vocab(testing_data, embed_size=embed_size)\n",
        "\n",
        "\n",
        "        # ---------------------------  \n",
        "        # testing\n",
        "        # ---------------------------\n",
        "        correct_sense = 0\n",
        "        sense_accuracy = 0\n",
        "        \n",
        "        # initialize the loss for testing\n",
        "        vloss_sum = 0\n",
        "        t0 = time.time()\n",
        "\n",
        "        predictions = {}\n",
        "\n",
        "        # set model to eval mode to ignore updating the weights of the model\n",
        "        self.model.eval()\n",
        "\n",
        "        # send the model to GPU\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # do not calculate gradients while evaluating\n",
        "        with torch.set_grad_enabled(False):\n",
        "\n",
        "            for batch in testing_generator:\n",
        "                #print(\"Batches for testing\")\n",
        "                #print(\"#\" * 100)\n",
        "\n",
        "                for id, local_batch, local_labels, local_synsets, local_idx in batch:\n",
        "\n",
        "                    # Transform list(<string>) to Tensor(<Tensor>)\n",
        "                    print(local_idx)\n",
        "                    print(\"Input Sentence\")\n",
        "                    print(local_batch)\n",
        "                    input_words = local_batch\n",
        "                    local_batch = numericalize(local_batch, VOCAB)\n",
        "\n",
        "#                     print(type(local_batch), local_batch)\n",
        "\n",
        "                    # the evaluation datasets may contain more than 1 tag\n",
        "                    if len(local_idx) > 1:\n",
        "                      # choose first synset\n",
        "                      local_idx = [l[0] for l in local_idx]\n",
        "                      print(\"Local_idx\", local_idx)\n",
        "                      #local_labels = local_labels[0]\n",
        "\n",
        "                    extract_labels = spatial_tags[local_idx]\n",
        "                    print(\"extract labels\", extract_labels)\n",
        "                    local_labels = [torch.tensor(lab) for lab in extract_labels]\n",
        "\n",
        "\n",
        "\n",
        "                    # Transform List(<Tensor>) to Tensor(<Tensor>)\n",
        "                    local_labels = torch.stack(local_labels)\n",
        "                    print(\"Labels:\")\n",
        "                    print(local_synsets)\n",
        "                    \n",
        "                    # Transfer to GPU\n",
        "                    local_batch, local_labels = local_batch.to(self.device), local_labels.to(self.device)\n",
        "\n",
        "                    # Model computations\n",
        "                    out = self.model(local_batch)\n",
        "                    predictions[id] = out\n",
        "\n",
        "                    # During validation and testing, I want to be less strict.\n",
        "                    # So, if a point resides within the label sphere, the sense is correctly identified.\n",
        "                    loss = geometric_loss(out, local_labels, include_r=True)\n",
        "\n",
        "                    vloss_sum += loss.item()                  \n",
        "\n",
        "                    testing_loss = vloss_sum / len(local_batch)\n",
        "                    print(\"testing_loss = \", type(testing_loss), testing_loss)\n",
        "                    \n",
        "                    history['testing_loss'].append(testing_loss)\n",
        "\n",
        "        return predictions #history\n",
        "\n",
        "    def tag(self, sentence, embed_size, target_vocab, spatial_tags, k, annotation_path, traindata_name, chkpt_name):\n",
        "        \"\"\"\n",
        "        tags the words of any sentence to their corresponding senses. \n",
        "        \"\"\"\n",
        "\n",
        "        print(\"Initial Input: \", sentence)\n",
        "        \n",
        "        if isinstance(sentence, str):\n",
        "            # preprocess the sentence, such that the lemmatized sentence is returned\n",
        "            lemm_sentence = preprocess(sentence)\n",
        "            tokens = list(map(lambda x: x[0], lemm_sentence))\n",
        "            pos = list(map(lambda x: x[1], lemm_sentence))\n",
        "            \n",
        "        if isinstance(sentence, list):\n",
        "            lst2str = \" \".join(sentence)\n",
        "            lemm_sentence = preprocess(lst2str)\n",
        "            tokens = list(map(lambda x: x[0], lemm_sentence))\n",
        "            pos = list(map(lambda x: x[1], lemm_sentence))\n",
        "\n",
        "    \n",
        "        \n",
        "        N = len(tokens)\n",
        "        tags = '?' * N\n",
        "        print(\"Lemmatized Sentence: \", tokens)\n",
        "               \n",
        "        data = tokens\n",
        "        \n",
        "        # words embeddings\n",
        "        vocab, wmat = load_vocab(data, embed_size)\n",
        "        \n",
        "        # numericalize words\n",
        "        num_data = numericalize(data, vocab)\n",
        "        \n",
        "        num_data = num_data.to(self.device)\n",
        "        \n",
        "        out = self.model(num_data)\n",
        "        \n",
        "        distances = []\n",
        "        predicted_indices = []\n",
        "\n",
        "        for i, word_tag in enumerate(out):\n",
        "            sentence_name = \"_\".join(tokens)\n",
        "            \n",
        "            if not os.path.exists(os.path.join(annotation_path, traindata_name, chkpt_name, sentence_name)):\n",
        "                os.mkdir(os.path.join(annotation_path, traindata_name, chkpt_name, sentence_name))\n",
        "\n",
        "             \n",
        "            outfile = os.path.join(annotation_path, traindata_name, chkpt_name, sentence_name, f'{i}.txt')\n",
        "\n",
        "            if os.path.exists(outfile):\n",
        "                print(\"Existing file in: \", outfile)\n",
        "                print(\"Resume Results ...\")\n",
        "                i += 1\n",
        "                \n",
        "            else: \n",
        "                  f = open(outfile, 'w+')\n",
        "                  print(\"Original Document:\", file=f)\n",
        "                  print(sentence, file=f)\n",
        "                  print(\" \", file=f)\n",
        "                  lemmatized_sentence = tokens\n",
        "                  for lemma, p in zip(lemmatized_sentence, pos):\n",
        "                    print(f'{lemma}, {p}', file=f)\n",
        "                  print('-' * 100, file=f)\n",
        "                  print(' ', file=f)\n",
        "                  \n",
        "                                      \n",
        "                  vindices, vmat, vdist = vicinity_matrix(spatial_params=word_tag,\n",
        "                                                          pos=pos[i],\n",
        "                                                          target_vocab=target_vocab,\n",
        "                                                          spatial_tags=spatial_tags, \n",
        "                                                          pos_tags=pos_tags,\n",
        "                                                          k=k, device=self.device)\n",
        "                  predicted_indices.append(vindices)\n",
        "                  distances.append(vdist)\n",
        "                \n",
        "                  decode_tag(tokens=[lemmatized_sentence[i]], sentag_idx=[vindices], sentag_dist=[vdist], file=f, true_synsets=None)\n",
        "\n",
        "          \n",
        "        return tokens, predicted_indices, distances\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNXr_LtJxmqX"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF9Tad7v-Vpe"
      },
      "source": [
        "### create sample, small splittings to test if the model overfits (to see if it learns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuy0IdZH-VRJ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qu3rfn8coJb5"
      },
      "outputs": [],
      "source": [
        "# superficial_splittings = splittings #[\"train\"][:20]\n",
        "# print(superficial_splittings)\n",
        "# superficial_splittings[\"train\"] = [splittings[\"train\"][7]] * 50\n",
        "# # superficial_splittings[\"validate\"] = ['115535'] * 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbAYxRI_5pfl"
      },
      "outputs": [],
      "source": [
        "# superficial_splittings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45N9fFezIhuR"
      },
      "outputs": [],
      "source": [
        "# path2data=resources_path + pwngc_path + \"/indexed_pwngc_id.pt\"\n",
        "# torch.load(path2data)['181688']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dr99wikNh0Q"
      },
      "outputs": [],
      "source": [
        "# superficial_splittings[\"validate\"] = ['181688'] * 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqhaTfNQQ-MA"
      },
      "outputs": [],
      "source": [
        "# superficial_splittings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4p7ggSz-iHT"
      },
      "source": [
        " # Begin Training 1\n",
        " ## 25 epochs, 4 attention heads, 8 nlayers, 200 hidden layers\n",
        " ## checkpoint name: training_state.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5Ne1kewYM9h"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "print(use_cuda)\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "T = RegTagger(use_cuda=use_cuda, device=device)\n",
        "\n",
        "train_history = T.train(batch_size=64, \n",
        "                        num_workers=0, \n",
        "                        max_epochs=25, \n",
        "                        splittings=splittings, \n",
        "                        path2data=idx_file, \n",
        "                        data=list(datasetID.values()), \n",
        "                        embed_size=300, \n",
        "                        target_vocab=target_VOCAB, \n",
        "                        spatial_tags=SPATIAL_TAGS,\n",
        "                        chkpt_path=CHECKPOINT_PATH, \n",
        "                        chkpt_name='training_state.pt',\n",
        "                        optimizer=\"adam\",\n",
        "                        d_model=300, d_hid=200, nlayers=8, nhead=4, dropout=0.2,\n",
        "                        lr=1e-2, gamma=0.95, \n",
        "                        shuffle=True\n",
        "                        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGXtkBRB1QNp"
      },
      "source": [
        "## Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BD1c3TzE2Y-7"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "T = RegTagger(use_cuda=use_cuda, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_Z8TVmG1PA3"
      },
      "outputs": [],
      "source": [
        "if os.path.exists(os.path.join(CHECKPOINT_PATH, 'training_state.pt')):\n",
        "      print(\"Resuming Training\")\n",
        "      checkpoint = torch.load(os.path.join(CHECKPOINT_PATH, 'training_state.pt'), map_location=DEVICE)\n",
        "      starting_epoch = checkpoint['epoch']\n",
        "      loss_sum = checkpoint['loss']\n",
        "      history = checkpoint['history']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnYVwQ7qkp1f"
      },
      "outputs": [],
      "source": [
        "train_history = history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXpqJ6iMxd_f"
      },
      "outputs": [],
      "source": [
        "train_history.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWsO0JcoxznI"
      },
      "outputs": [],
      "source": [
        "len(train_history[\"train_loss\"]) # this comes from  #training_sentences * #epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYOr0uuFx6fm"
      },
      "outputs": [],
      "source": [
        "len(train_history[\"validation_loss\"]) # this comes from #validation_sentences * #epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpLR79RK1NHc"
      },
      "outputs": [],
      "source": [
        "# now I need to rearrange the training_loss function, by adding every 176430 losses together. At the end, 25 values come out\n",
        "nb_epochs = 25\n",
        "nb_train_sentences = 176430\n",
        "new_train_loss = np.array_split(np.array(train_history[\"train_loss\"]), nb_epochs)\n",
        "print(type(new_train_loss[0]))\n",
        "print(len(new_train_loss))\n",
        "\n",
        "tloss = [(np.sum(loss, 0)/nb_train_sentences) for loss in new_train_loss]\n",
        "print(tloss[0])\n",
        "print(len(tloss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1b6ZMvL2at6"
      },
      "outputs": [],
      "source": [
        "# now I need to rearrange the validation_loss function, by adding every  losses together. At the end, 25 values come out\n",
        "nb_epochs = 25\n",
        "nb_val_sentences = 9285\n",
        "new_val_loss = np.array_split(np.array(train_history[\"validation_loss\"]), nb_epochs)\n",
        "vloss = [np.sum(loss, 0)/nb_val_sentences for loss in new_val_loss]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxx9IHXuyplR"
      },
      "outputs": [],
      "source": [
        "# Get training and test loss histories\n",
        "training_loss = tloss # train_history['train_loss']\n",
        "#test_loss = vloss #train_history['validation_loss']\n",
        "\n",
        "# Create count of the number of epochs\n",
        "epoch_count = range(1, len(tloss)+1)\n",
        "\n",
        "# Visualize loss history\n",
        "plt.plot(epoch_count, training_loss, 'r--')#, alpha= 0.3)\n",
        "#plt.plot(epoch_count, test_loss, 'b-')#, alpha = 0.2)\n",
        "#plt.legend(['Training Loss', 'Validation Loss'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOROGfusNCEC"
      },
      "outputs": [],
      "source": [
        "# Get training and test loss histories\n",
        "#training_loss = tloss # train_history['train_loss']\n",
        "test_loss = vloss #train_history['validation_loss']\n",
        "\n",
        "# Create count of the number of epochs\n",
        "epoch_count = range(1, len(vloss)+1)\n",
        "\n",
        "# Visualize loss history\n",
        "#plt.plot(epoch_count, training_loss, 'r--')#, alpha= 0.3)\n",
        "plt.plot(epoch_count, test_loss, 'b-')#, alpha = 0.2)\n",
        "#plt.legend(['Training Loss', 'Validation Loss'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVIabHpnOdTl"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Plot linear sequence, and set tick labels to the same color\n",
        "ax.plot(tloss, color='red')\n",
        "ax.tick_params(axis='y', labelcolor='red')\n",
        "ax.set_ylabel(\"Training loss\", fontweight='bold')\n",
        "\n",
        "# Generate a new Axes instance, on the twin-X axes (same position)\n",
        "ax2 = ax.twinx()\n",
        "\n",
        "# Plot exponential sequence, set scale to logarithmic and change tick color\n",
        "ax2.plot(vloss, color='blue', linestyle='dashed', alpha=0.5)\n",
        "#ax2.set_yscale('log')\n",
        "ax2.tick_params(axis='y', labelcolor='b')\n",
        "ax2.set_ylabel(\"Validation loss\", fontweight='bold')\n",
        "\n",
        "ax.set_xlabel(\"Number of epochs\", fontweight='bold')\n",
        "ax.set_title(\"Training History PWNGC\\ndhid=200, nlayers=8, nhead=4\", fontweight='bold')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.savefig(resources_path + pwngc_path + '/pwngc_train_history_4heads_25epochs.png')\n",
        "\n",
        "files.download(resources_path + pwngc_path + '/pwngc_train_history_4heads_25epochs.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLf9RL5EaTGz"
      },
      "source": [
        "Analysis of this training history (dhid=200, nhead=4, epochs=25):\n",
        "- unstable training, the training and validation losses are oscillating. \n",
        "  - why?\n",
        "    1. the network size is small. This causes that each processed batch changes the weights in the network excessively, resulting in a weak representation of the data.\n",
        "    2. saturation caused by sigmoid. I use Sigmoid to output a list of params between 0 and 1, then I calculate the logistic scaling to compare to the true label. \n",
        "    3. the learning rate choice: if it is too small, it might get stuck in the local minima, if the value is too big, it makes the network unstable with oscillating loss.\n",
        "    4. the depth of the network is not sufficient to learn patterns. \n",
        " \n",
        "- all of these causes apply to our settings.\n",
        "  - we need to increase the network size\n",
        "\n",
        "- Also, there might be some special issues with PWNGC since the dataset is very diverse, and do not contain much redundant senses (highly imbalanced). When the NN updates its params based on a batch, those parameters do not necessarily hold for the newly introduced batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTJiDXaBPYcr"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "f, axes = plt.subplots(2, 1)\n",
        "x = range(1, 25+1)\n",
        "axes[0].plot(x, tloss)\n",
        "axes[0].set_ylabel('train_loss')\n",
        "\n",
        "axes[1].plot(x, vloss)\n",
        "axes[1].set_ylabel('validation_loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uD7nEfCX871Z"
      },
      "outputs": [],
      "source": [
        "# store history in a pickle\n",
        "file4train_history = os.path.join(resources_path, pwngc_path, \"/train_hist_pwngc_25epochs_4heads.pkl\")\n",
        "\n",
        "open_file = open(file4train_history, \"wb\")\n",
        "pickle.dump(train_history, open_file)\n",
        "open_file.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mByqtQACYM9i"
      },
      "outputs": [],
      "source": [
        "sentence = ['run'] \n",
        "tokens, sentag_idx, sentag_dist = T.tag(sentence, 300, target_VOCAB, SPATIAL_TAGS, 5)\n",
        "decode_tag(tokens, sentag_idx, sentag_dist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hD2OIzBA6b1"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def parse_testing_data(file_path): # file_path = os.path.join(TESTING_PATH, \"/senseval2.pt\")\n",
        "#   \"\"\"\n",
        "#   parses the testing datasets, and prepares it for training\n",
        "#   \"\"\"\n",
        "\n",
        "#   data = torch.load(file_path)\n",
        " \n",
        "#   extracted_data = []\n",
        "\n",
        "#   for i, sent in enumerate(data):\n",
        "#       tmp_lemma = []\n",
        "#       tmp_tag = []\n",
        "#       tmp_synset = []\n",
        "#       tmp_idx = []\n",
        "#       for j, word in enumerate(sent): \n",
        "#         tmp_lemma.append(word[1])\n",
        "#         tmp_tag.append(word[5])\n",
        "#         tmp_synset.append(word[3])\n",
        "#         tmp_idx.append(word[4])\n",
        "#       tmp_sent = [tmp_lemma, tmp_tag, tmp_synset, tmp_idx]\n",
        "#       extracted_data.append(tmp_sent)\n",
        "#   return extracted_data"
      ],
      "metadata": {
        "id": "jPssi9CSPCwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8teAH946BQx-"
      },
      "outputs": [],
      "source": [
        "# # Senseval2\n",
        "# senseval2_file = \"/senseval2.pt\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrVbte2MKnDQ"
      },
      "outputs": [],
      "source": [
        "# senseval2 = torch.load(TESTING_PATH + \"/senseval2.pt\")\n",
        "# senseval2[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLWsbRB1g6KD"
      },
      "outputs": [],
      "source": [
        "# # extract needed data from senseval\n",
        "# senseval2_ = []\n",
        "# for i in range(len(senseval2)):\n",
        "#     sent = senseval2[i]\n",
        "#     tmp_lemma = []\n",
        "#     tmp_tag = []\n",
        "#     tmp_synset = []\n",
        "#     tmp_idx = []\n",
        "#     for j in range(len(sent)):\n",
        "#       tmp_lemma.append(sent[j][1])\n",
        "#       tmp_tag.append(sent[j][5])\n",
        "#       tmp_synset.append(sent[j][3])\n",
        "#       tmp_idx.append(sent[j][4])\n",
        "#     tmp_sent = [tmp_lemma, tmp_tag, tmp_synset, tmp_idx]\n",
        "#     senseval2_.append(tmp_sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOlX-EjujSGt"
      },
      "outputs": [],
      "source": [
        "# senseval2_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dS9Pf8l6Ixjy"
      },
      "outputs": [],
      "source": [
        "senseval2 = parse_testing_data(os.path.join(TESTING_PATH, '/senseval2.pt'))\n",
        "senseval2_id = data_id(senseval2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8vfSJYcgo1b"
      },
      "outputs": [],
      "source": [
        "senseval2_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoAmjXSwOY0l"
      },
      "outputs": [],
      "source": [
        "torch.save(senseval2_id, os.path.join(TESTING_PATH, \"/senseval2_ID.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlFVHxyfHRqk"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrqrgbyt9pbg"
      },
      "outputs": [],
      "source": [
        "params = {'batch_size': 5,\n",
        "                  'shuffle': True,\n",
        "                  'collate_fn': lambda x: x,\n",
        "                  'num_workers': 0} \n",
        "                  # 'pin_memory': pin_memory}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NywrtyKTTWeb"
      },
      "outputs": [],
      "source": [
        "list_ids = list(senseval2_id.keys())\n",
        "path2data = TESTING_PATH + '/senseval2_ID.pt'\n",
        "testing_set = TestingDataset(list_ids, path2data)\n",
        "testing_generator = DataLoader(testing_set, **params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzQJcd-Zc-Mj"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sUnphV4UV8O"
      },
      "outputs": [],
      "source": [
        "for batch in testing_generator:\n",
        "  # print(type(batch[0][0]), batch[0])\n",
        "  # print(batch[1])\n",
        "  # print(batch[2])\n",
        "  # print(batch[3])\n",
        "  # print()\n",
        "    for sentence, label, syn, idx in batch:\n",
        "       print(sentence, label, syn, idx)\n",
        "       break\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4VDlsGZ1KQT"
      },
      "outputs": [],
      "source": [
        "test_history = T.test(testing_data=list(senseval2_id.values()), \n",
        "                      splittings=list(senseval2_id.keys()),\n",
        "                      path=TESTING_PATH + '/senseval2_ID.pt',  \n",
        "                      embed_size=300, \n",
        "                      batch_size=242, \n",
        "                      num_workers=0, \n",
        "                      target_vocab=target_VOCAB, \n",
        "                      spatial_tags=SPATIAL_TAGS, \n",
        "                      k=5,\n",
        "                      shuffle=True)       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kB_V_Tri_nhK"
      },
      "outputs": [],
      "source": [
        "senseval2_id['0']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAs_tzdQ_tOd"
      },
      "outputs": [],
      "source": [
        "sentence = senseval2_id[\"0\"][0]\n",
        "tokens, sentag_idx, sentag_dist = T.tag(sentence, 300, target_VOCAB, SPATIAL_TAGS, k=3)\n",
        "decode_tag(tokens, sentag_idx, sentag_dist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArUenr33-YLz"
      },
      "source": [
        "# Begin Training 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gmQN1gt-boY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "outputId": "f8a157c6-54f0-4a02-82ec-3e023562e2bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "cuda\n",
            "[['necessary', 'means', 'know-how', 'authority'], ['necessary.a.01', 'means.n.01', 'know-how.n.01', 'authority.n.01'], [tensor([5.0801e+04, 1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]), tensor([7.9562e+04, 1.1174e+05, 9.8310e+01, 9.8013e+04, 0.0000e+00, 1.8500e+01]), tensor([1.7459e+05, 1.4268e+05, 1.0717e+02, 7.1890e+04, 0.0000e+00, 1.0650e+02]), tensor([9.4484e+04, 3.7587e+04, 1.0434e+02, 1.9497e+05, 0.0000e+00, 7.5000e+00])], [50801, 79562, 174594, 94484]]\n",
            "\n",
            "Count results:\n",
            "nb_words_training = 506146\n",
            "nb_train_sentences = 176430\n",
            "nb_words_validation = 26674\n",
            "\n",
            "Resuming Training in the file pwngc_training_state_20epochs_8heads_6layers_2048hidden.pt ...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training ...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-c7eef47524fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m                         \u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_hid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnhead\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                         \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                         )\n",
            "\u001b[0;32m<ipython-input-40-98fef9a9fbfc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, batch_size, num_workers, max_epochs, splittings, path2data, data, embed_size, target_vocab, spatial_tags, chkpt_path, chkpt_name, optimizer, k, d_model, d_hid, nlayers, nhead, dropout, lr, gamma, shuffle)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;31m# Transform list(<string>) to Tensor(<Tensor>) after numerecalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0minput_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mlocal_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVOCAB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                     \u001b[0mextract_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspatial_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlocal_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-3e87d69bf2d4>\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(tokens_list, vocab)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \"\"\"\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mstr2num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mnum_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-3e87d69bf2d4>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \"\"\"\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mstr2num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mnum_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "print(use_cuda)\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "#torch.backends.cudnn.benchmark = True\n",
        "\n",
        "CHECKPOINT_NAME = 'pwngc_training_state_20epochs_8heads_6layers_2048hidden'\n",
        "\n",
        "T = RegTagger(use_cuda=use_cuda, device=device)\n",
        "\n",
        "train_history = T.train(batch_size=64, \n",
        "                        num_workers=0, \n",
        "                        max_epochs=20, \n",
        "                        splittings=splittings, \n",
        "                        path2data=idx_file, \n",
        "                        data=list(datasetID.values()), \n",
        "                        embed_size=300, \n",
        "                        target_vocab=target_VOCAB, \n",
        "                        spatial_tags=SPATIAL_TAGS,\n",
        "                        chkpt_path=CHECKPOINT_PATH, \n",
        "                        chkpt_name=CHECKPOINT_NAME+ '.pt',\n",
        "                        optimizer=\"adam\",\n",
        "                        d_model=300, d_hid=2048, nlayers=6, nhead=4, dropout=0.1,\n",
        "                        lr=1e-3, gamma=0.95, \n",
        "                        shuffle=True\n",
        "                        )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## plots"
      ],
      "metadata": {
        "id": "n1QxroZfcK0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Plot linear sequence, and set tick labels to the same color\n",
        "ax.plot(train_history[\"train_loss\"], color='red')\n",
        "ax.tick_params(axis='y', labelcolor='red')\n",
        "ax.set_ylabel(\"Training loss\", fontweight='bold')\n",
        "\n",
        "# Generate a new Axes instance, on the twin-X axes (same position)\n",
        "ax2 = ax.twinx()\n",
        "\n",
        "# Plot exponential sequence, set scale to logarithmic and change tick color\n",
        "ax2.plot(train_history[\"validation_loss\"], color='blue', linestyle='dashed', alpha=0.5)\n",
        "#ax2.set_yscale('log')\n",
        "ax2.tick_params(axis='y', labelcolor='b')\n",
        "ax2.set_ylabel(\"Validation loss\", fontweight='bold')\n",
        "\n",
        "ax.set_xlabel(\"Number of epochs\", fontweight='bold')\n",
        "ax.set_title(\"Training History PWNGC\\ndhid=2048, nlayers=6, nhead=4\", fontweight='bold')\n",
        "\n",
        "fig1 = plt.gcf()\n",
        "plt.show()\n",
        "plt.draw()\n",
        "\n",
        "\n",
        "\n",
        "fig1.savefig(resources_path + pwngc_path + '/pwngc_train_history_4heads_6layers_20epochs.png')\n",
        "\n",
        "#plt.download(resources_path + pwngc_path + '/pwngc_train_history_4heads_6layers_20epochs.png')"
      ],
      "metadata": {
        "id": "RWQgWCDmeXA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_history"
      ],
      "metadata": {
        "id": "kmKX2kiGfcE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "f, axes = plt.subplots(2, 1)\n",
        "x = range(1, 20+1)\n",
        "axes[0].plot(x, train_history[\"train_loss\"])\n",
        "axes[0].set_ylabel('train_loss')\n",
        "\n",
        "axes[1].plot(x, train_history[\"validation_loss\"])\n",
        "axes[1].set_ylabel('validation_loss')"
      ],
      "metadata": {
        "id": "ubre2SZKeuHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm_train_loss = train_history[\"train_loss\"] / np.sum(train_history[\"train_loss\"])\n",
        "norm_valid_loss = train_history[\"validation_loss\"] / np.sum(train_history[\"validation_loss\"])"
      ],
      "metadata": {
        "id": "s9rsyfkuhsgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Plot linear sequence, and set tick labels to the same color\n",
        "ax.plot(norm_train_loss, color='red')\n",
        "ax.tick_params(axis='y', labelcolor='red')\n",
        "ax.set_ylabel(\"Training loss\", fontweight='bold')\n",
        "\n",
        "# Generate a new Axes instance, on the twin-X axes (same position)\n",
        "ax2 = ax.twinx()\n",
        "\n",
        "# Plot exponential sequence, set scale to logarithmic and change tick color\n",
        "ax2.plot(norm_valid_loss, color='blue', linestyle='dashed', alpha=0.5)\n",
        "#ax2.set_yscale('log')\n",
        "ax2.tick_params(axis='y', labelcolor='b')\n",
        "ax2.set_ylabel(\"Validation loss\", fontweight='bold')\n",
        "\n",
        "ax.set_xlabel(\"Number of epochs\", fontweight='bold')\n",
        "ax.set_title(\"Training History PWNGC\\ndhid=2048, nlayers=6, nhead=4\", fontweight='bold')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.savefig(resources_path + pwngc_path + '/pwngc_train_history_4heads_6layers_20epochs.svg', format='svg')\n",
        "\n",
        "files.download(resources_path + pwngc_path + '/pwngc_train_history_4heads_6layers_20epochs.png')"
      ],
      "metadata": {
        "id": "ReSCqoPKh-GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training interpretation:\n",
        "- the training loss is still oscillating \n",
        "- the validation loss is almost stable and fluctuates in epoch 11\n",
        "- training loss increases a bit, while validation loss is lower. However, when the training loss decreases, the validation loss increases\n",
        "- why is this happening?\n",
        "  1. the training data is not enough for training/validation, and not suitable, since PWNGC has a great coverage of WordNet synsets, however, it does not provide sufficient instances per synset\n",
        "  2. the model is overfitting --> I need to set Dropout higher than that.\n",
        "- What is the next step?\n",
        "  - train on SemCor and PWNGC+Semcor"
      ],
      "metadata": {
        "id": "_b7j3IRiibMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "sLQmtiigcXZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check if the path to testing set exists, then ..."
      ],
      "metadata": {
        "id": "nTJpWBzIia3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset_name = 'senseval2'\n",
        "test_dataset = 'senseval2.pt'\n",
        "test_dataset_id = 'senseval2_ID.pt'\n",
        "test_full_sent = 'sent_seneval2.pt'"
      ],
      "metadata": {
        "id": "KPH2e3L__N3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(os.path.join(TESTING_PATH, test_dataset_id)):\n",
        "  print(\"Found testing dataset id at: {}\".format(os.path.join(TESTING_PATH, test_dataset_id)))\n",
        "  senseval2_id = torch.load(os.path.join(TESTING_PATH, test_dataset_id))\n",
        "else:\n",
        "  senseval2 = parse_testing_data(os.path.join(TESTING_PATH, test_dataset))\n",
        "  senseval2_id = data_id(senseval2)\n",
        "  torch.save(senseval2_id, os.path.join(TESTING_PATH, test_dataset_id))\n"
      ],
      "metadata": {
        "id": "1TkpfiWrevn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_history = T.test(testing_data=list(senseval2_id.values()), \n",
        "                      splittings=list(senseval2_id.keys()),\n",
        "                      path= os.path.join(TESTING_PATH, test_dataset_id),  \n",
        "                      embed_size=300, \n",
        "                      batch_size=242, \n",
        "                      num_workers=0, \n",
        "                      target_vocab=target_VOCAB, \n",
        "                      spatial_tags=SPATIAL_TAGS, \n",
        "                      k=5,\n",
        "                      shuffle=True) \n"
      ],
      "metadata": {
        "id": "8ON6PVrtgOzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_predictions = torch.save(test_predictions, os.path.join(RESULTS_PATH, traindata_name, CHECKPOINT_NAME, test_dataset_name, test_dataset))"
      ],
      "metadata": {
        "id": "xUr7NLIp_hlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = torch.load(os.path.join(RESULTS_PATH, traindata_name, CHECKPOINT_NAME, test_dataset_name, test_dataset))"
      ],
      "metadata": {
        "id": "4p5BstMa_-nI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_sentences = torch.load(os.path.join(TESTING_PATH, test_full_sent))\n",
        "# in case the computation stops, keep track of existing files\n",
        "processed_instances = save_results(predictions=preds, results_path=RESULTS_PATH, traindata_name=traindata_name, chkpt_name=CHECKPOINT_NAME, testset_name=test_dataset_name, \n",
        "                testset_id=senseval2_id, document=test_full_sent, \n",
        "                target_vocab=target_VOCAB, spatial_tags=SPATIAL_TAGS, k=3)"
      ],
      "metadata": {
        "id": "kqsscI7bigDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tagging/Annotate any sentence"
      ],
      "metadata": {
        "id": "uYnJiXIT8vrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens, predicted_indices, distances = T.tag(sentence=\"The boy is running towards the tree.\", embed_size=300,\n",
        "                                             target_vocab=target_VOCAB, spatial_tags=SPATIAL_TAGS, k=3,\n",
        "                                             annotation_path=ANNOTATION_PATH, traindata_name=traindata_name, chkpt_name=CHECKPOINT_NAME)\n"
      ],
      "metadata": {
        "id": "nF4Z_mhP8zxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Begin Training 3"
      ],
      "metadata": {
        "id": "lwRg8v9hJ7jV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpsXjB5A0ylw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 787
        },
        "outputId": "45b5454d-4cf8-40c8-d286-fd420eef06de"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "cuda\n",
            "[['necessary', 'means', 'know-how', 'authority'], ['necessary.a.01', 'means.n.01', 'know-how.n.01', 'authority.n.01'], [tensor([5.0801e+04, 1.3514e+05, 2.5010e+01, 6.4176e+04, 9.0000e+01, 5.0000e-01]), tensor([7.9562e+04, 1.1174e+05, 9.8310e+01, 9.8013e+04, 0.0000e+00, 1.8500e+01]), tensor([1.7459e+05, 1.4268e+05, 1.0717e+02, 7.1890e+04, 0.0000e+00, 1.0650e+02]), tensor([9.4484e+04, 3.7587e+04, 1.0434e+02, 1.9497e+05, 0.0000e+00, 7.5000e+00])], [50801, 79562, 174594, 94484]]\n",
            "\n",
            "Count results:\n",
            "nb_words_training = 506146\n",
            "nb_train_sentences = 176430\n",
            "nb_words_validation = 26674\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training ...\n",
            "Validation ...\n",
            "Epoch 1: train loss = 175722.4717, time = 3976.7163 (s)\n",
            "Training ...\n",
            "Validation ...\n",
            "Epoch 2: train loss = 175734.0089, time = 3883.5013 (s)\n",
            "Training ...\n",
            "Validation ...\n",
            "Epoch 3: train loss = 175759.8310, time = 3833.5040 (s)\n",
            "Training ...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-a81cab191547>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m                         \u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_hid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnhead\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                         \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                         )\n",
            "\u001b[0;32m<ipython-input-40-98fef9a9fbfc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, batch_size, num_workers, max_epochs, splittings, path2data, data, embed_size, target_vocab, spatial_tags, chkpt_path, chkpt_name, optimizer, k, d_model, d_hid, nlayers, nhead, dropout, lr, gamma, shuffle)\u001b[0m\n\u001b[1;32m    192\u001b[0m                     \u001b[0;31m# Model computations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                     \u001b[0;31m# out outputs the predicted spatial parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeometric_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmean_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-3444839a2479>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mencoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m    349\u001b[0m                            \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                            need_weights=False)[0]\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# feed forward block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[0;34m\"but got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "print(use_cuda)\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "#torch.backends.cudnn.benchmark = True\n",
        "\n",
        "CHECKPOINT_NAME = 'pwngc_training_state_20epochs_4heads_6layers_2048hid'\n",
        "\n",
        "T = RegTagger(use_cuda=use_cuda, device=device)\n",
        "\n",
        "train_history = T.train(batch_size=64, \n",
        "                        num_workers=0, \n",
        "                        max_epochs=20, \n",
        "                        splittings=splittings, \n",
        "                        path2data=idx_file, \n",
        "                        data=list(datasetID.values()), \n",
        "                        embed_size=300, \n",
        "                        target_vocab=target_VOCAB, \n",
        "                        spatial_tags=SPATIAL_TAGS,\n",
        "                        chkpt_path=CHECKPOINT_PATH, \n",
        "                        chkpt_name=CHECKPOINT_NAME+ '.pt',\n",
        "                        optimizer=\"adam\",\n",
        "                        d_model=300, d_hid=2048, nlayers=6, nhead=4, dropout=0.2,\n",
        "                        lr=1e-3, gamma=0.95, \n",
        "                        shuffle=True\n",
        "                        )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## plots"
      ],
      "metadata": {
        "id": "rwT6lLYt0yl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Plot linear sequence, and set tick labels to the same color\n",
        "ax.plot(train_history[\"train_loss\"], color='red')\n",
        "ax.tick_params(axis='y', labelcolor='red')\n",
        "ax.set_ylabel(\"Training loss\", fontweight='bold')\n",
        "\n",
        "# Generate a new Axes instance, on the twin-X axes (same position)\n",
        "ax2 = ax.twinx()\n",
        "\n",
        "# Plot exponential sequence, set scale to logarithmic and change tick color\n",
        "ax2.plot(train_history[\"validation_loss\"], color='blue', linestyle='dashed', alpha=0.5)\n",
        "#ax2.set_yscale('log')\n",
        "ax2.tick_params(axis='y', labelcolor='b')\n",
        "ax2.set_ylabel(\"Validation loss\", fontweight='bold')\n",
        "\n",
        "ax.set_xlabel(\"Number of epochs\", fontweight='bold')\n",
        "ax.set_title(\"Training History PWNGC\\ndhid=2048, nlayers=6, nhead=4\", fontweight='bold')\n",
        "\n",
        "fig1 = plt.gcf()\n",
        "plt.show()\n",
        "plt.draw()\n",
        "\n",
        "\n",
        "\n",
        "fig1.savefig(resources_path + pwngc_path + '/pwngc_train_history_4heads_6layers_20epochs.png')\n",
        "\n",
        "#plt.download(resources_path + pwngc_path + '/pwngc_train_history_4heads_6layers_20epochs.png')"
      ],
      "metadata": {
        "id": "12-1dJpC0yl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_history"
      ],
      "metadata": {
        "id": "p554HOJ80yl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "f, axes = plt.subplots(2, 1)\n",
        "x = range(1, 20+1)\n",
        "axes[0].plot(x, train_history[\"train_loss\"])\n",
        "axes[0].set_ylabel('train_loss')\n",
        "\n",
        "axes[1].plot(x, train_history[\"validation_loss\"])\n",
        "axes[1].set_ylabel('validation_loss')"
      ],
      "metadata": {
        "id": "K5zwOVFp0yl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm_train_loss = train_history[\"train_loss\"] / np.sum(train_history[\"train_loss\"])\n",
        "norm_valid_loss = train_history[\"validation_loss\"] / np.sum(train_history[\"validation_loss\"])"
      ],
      "metadata": {
        "id": "hGwMXseE0yl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Plot linear sequence, and set tick labels to the same color\n",
        "ax.plot(norm_train_loss, color='red')\n",
        "ax.tick_params(axis='y', labelcolor='red')\n",
        "ax.set_ylabel(\"Training loss\", fontweight='bold')\n",
        "\n",
        "# Generate a new Axes instance, on the twin-X axes (same position)\n",
        "ax2 = ax.twinx()\n",
        "\n",
        "# Plot exponential sequence, set scale to logarithmic and change tick color\n",
        "ax2.plot(norm_valid_loss, color='blue', linestyle='dashed', alpha=0.5)\n",
        "#ax2.set_yscale('log')\n",
        "ax2.tick_params(axis='y', labelcolor='b')\n",
        "ax2.set_ylabel(\"Validation loss\", fontweight='bold')\n",
        "\n",
        "ax.set_xlabel(\"Number of epochs\", fontweight='bold')\n",
        "ax.set_title(\"Training History PWNGC\\ndhid=2048, nlayers=6, nhead=4\", fontweight='bold')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.savefig(resources_path + pwngc_path + '/pwngc_train_history_4heads_6layers_20epochs.svg', format='svg')\n",
        "\n",
        "files.download(resources_path + pwngc_path + '/pwngc_train_history_4heads_6layers_20epochs.png')"
      ],
      "metadata": {
        "id": "KNlwcjcu0yl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training interpretation:\n",
        "- the training loss is still oscillating \n",
        "- the validation loss is almost stable and fluctuates in epoch 11\n",
        "- training loss increases a bit, while validation loss is lower. However, when the training loss decreases, the validation loss increases\n",
        "- why is this happening?\n",
        "  1. the training data is not enough for training/validation, and not suitable, since PWNGC has a great coverage of WordNet synsets, however, it does not provide sufficient instances per synset\n",
        "  2. the model is overfitting --> I need to set Dropout higher than that.\n",
        "- What is the next step?\n",
        "  - train on SemCor and PWNGC+Semcor"
      ],
      "metadata": {
        "id": "Sf_ZQA_o0yl8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "kp2YjtmG0yl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check if the path to testing set exists, then ..."
      ],
      "metadata": {
        "id": "4fUChTFE0yl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset_name = 'senseval2'\n",
        "test_dataset = 'senseval2.pt'\n",
        "test_dataset_id = 'senseval2_ID.pt'\n",
        "test_full_sent = 'sent_seneval2.pt'"
      ],
      "metadata": {
        "id": "bj0bnEYb0yl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(os.path.join(TESTING_PATH, test_dataset_id)):\n",
        "  print(\"Found testing dataset id at: {}\".format(os.path.join(TESTING_PATH, test_dataset_id)))\n",
        "  senseval2_id = torch.load(os.path.join(TESTING_PATH, test_dataset_id))\n",
        "else:\n",
        "  senseval2 = parse_testing_data(os.path.join(TESTING_PATH, test_dataset))\n",
        "  senseval2_id = data_id(senseval2)\n",
        "  torch.save(senseval2_id, os.path.join(TESTING_PATH, test_dataset_id))\n"
      ],
      "metadata": {
        "id": "bh5risis0yl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_history = T.test(testing_data=list(senseval2_id.values()), \n",
        "                      splittings=list(senseval2_id.keys()),\n",
        "                      path= os.path.join(TESTING_PATH, test_dataset_id),  \n",
        "                      embed_size=300, \n",
        "                      batch_size=242, \n",
        "                      num_workers=0, \n",
        "                      target_vocab=target_VOCAB, \n",
        "                      spatial_tags=SPATIAL_TAGS, \n",
        "                      k=5,\n",
        "                      shuffle=True) \n"
      ],
      "metadata": {
        "id": "tzMYPsS00yl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_predictions = torch.save(test_predictions, os.path.join(RESULTS_PATH, traindata_name, CHECKPOINT_NAME, test_dataset_name, test_dataset))"
      ],
      "metadata": {
        "id": "R1pZ_-v-0yl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = torch.load(os.path.join(RESULTS_PATH, traindata_name, CHECKPOINT_NAME, test_dataset_name, test_dataset))"
      ],
      "metadata": {
        "id": "gUyXj9Ii0yl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_sentences = torch.load(os.path.join(TESTING_PATH, test_full_sent))\n",
        "# in case the computation stops, keep track of existing files\n",
        "processed_instances = save_results(predictions=preds, results_path=RESULTS_PATH, traindata_name=traindata_name, chkpt_name=CHECKPOINT_NAME, testset_name=test_dataset_name, \n",
        "                testset_id=senseval2_id, document=test_full_sent, \n",
        "                target_vocab=target_VOCAB, spatial_tags=SPATIAL_TAGS, k=3)"
      ],
      "metadata": {
        "id": "8rlKifXj0yl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tagging/Annotate any sentence"
      ],
      "metadata": {
        "id": "FB6MrWrw0yl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens, predicted_indices, distances = T.tag(sentence=\"The boy is running towards the tree.\", embed_size=300,\n",
        "                                             target_vocab=target_VOCAB, spatial_tags=SPATIAL_TAGS, k=3,\n",
        "                                             annotation_path=ANNOTATION_PATH, traindata_name=traindata_name, chkpt_name=CHECKPOINT_NAME)\n"
      ],
      "metadata": {
        "id": "xsH9PJHH0yl9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "yG45YqPiOwtZ",
        "rNHjzlbeyYk7",
        "AaNWxx1ZYM9J",
        "JXSQufUkYM9P",
        "L6PSPtIAvhvc",
        "4swbnCh2Cc7t",
        "sp-hIJf-YM9Z",
        "buL4R4UuYM9c",
        "0vR0knpCt5zf",
        "BF9Tad7v-Vpe",
        "HGXtkBRB1QNp",
        "6hD2OIzBA6b1"
      ],
      "name": "regressor_pwngc_index_based.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}